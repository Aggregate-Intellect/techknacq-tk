#!/usr/bin/env python
# -*- coding: utf-8 -*-

# T: Build Corpus
# Jonathan Gordon

import sys
import os
import codecs
import string
import re
import json
import requests
import nltk
import slate
import multiprocessing as mp

from math import log
from functools import partial
from collections import defaultdict, Counter
from noaho import NoAho
from bs4 import BeautifulSoup

import wikipedia as wiki
from py_bing_search import PyBingSearch

from t.lx import ScrabbleLexicon, StopLexicon


# Parameters

PROCESSES = int(.75 * mp.cpu_count())

SD_API_KEY = open(os.path.expanduser('~/.t/sd.txt')).read().strip()
BING_API_KEY = open(os.path.expanduser('~/.t/bing.txt')).read().strip()


###


def get_terms(dirname):
    """Return an ordered list of technical terms or names of research
    topics, based on citation graph density."""

    stop = StopLexicon()
    scrabble = ScrabbleLexicon()

    print '-- Reading corpus.'
    ngrams = defaultdict(set)
    citations = defaultdict(set)
    for i, docname in enumerate(os.listdir(dirname)):
        if i % 2000 == 0:
            print '   Document', i
        with codecs.open(os.path.join(dirname, docname), 'r', 'utf8') as f:
            j = json.load(f)
            doc_id = j['info']['id']
            if not 'title' in j['info']:
                print '   No title for', docname
                continue
            for ng in get_ngrams([j['info']['title']]):
                if not good_ngram(ng, scrabble, stop):
                    continue
                ngrams[ng].add(doc_id)
            for ref in j['references']:
                citations[doc_id].add(ref['id'])
                citations[ref['id']].add(doc_id)

    ngrams = filter_plurals(ngrams)

    ngram_counts = dict([(x, len(ngrams[x])) for x in ngrams])
    filtered = filter_subsumed(ngram_counts)

    ngrams = score_ngrams(ngrams, citations)
    ngrams = filter_subsumed(ngrams)

    return [' '.join(x) for x in sorted(ngrams, key=lambda x: ngrams[x],
                                        reverse=True) if x in filtered]


def get_abstracts(dirname, maxdocs=None):
    """Return a list of abstracts from the JSON documents in the corpus."""

    def get_abstract(j):
        """Return the abstract of the JSON document as a list of sentences. An
        abstract is assumed to be the section with the heading 'Abstract' or,
        if there isn't one, the first section."""

        for section in j['sections'][:3]:
            if 'heading' not in section or not section['heading']:
                continue
            if section['heading'].lower() == 'abstract':
                return section['text'][:25]
        if len(j['sections']) > 0:
            return j['sections'][0]['text'][:25]

        print '   No sections in', docname
        return []

    abstracts = []
    for i, docname in enumerate(os.listdir(dirname)):
        if i % 2000 == 0:
            print '   Document', i
        if maxdocs and i >= maxdocs:
            break
        with codecs.open(os.path.join(dirname, docname), 'r', 'utf8') as f:
            abstr = get_abstract(json.load(f))
            abstracts.append(abstr)
    return abstracts



def get_ngrams(l, min=1):
    """Return the set of all n-grams that occur in the input strings, at
    least 'min' times."""

    def everygrams(seq):
        """Return all possible n-grams generated from a sequence of items,
        as an iterator."""
        for n in range(1, len(seq) + 1):
            for ng in nltk.util.ngrams(seq, n):
                yield ng

    counts = defaultdict(int)
    for i, s in enumerate(l):
        if i % 2000 == 0 and i > 0:
            print '   Item', i

        tokens = nltk.tokenize.word_tokenize(s)

        # Preserve case for acronyms inside n-grams but lowercase everything
        # else.
        new_tokens = []
        for token in tokens:
            if token.isupper() and len(token) > 1:
                new_tokens.append(token)
            else:
                new_tokens.append(token.lower())
        tokens = new_tokens

        for ngram in everygrams(tokens):
            counts[ngram] += 1

    return dict([(x, counts[x]) for x in counts if counts[x] >= min])


def good_ngram(ng, scrabble, stop):
    """Check if the n-gram is good: It's not a single word that would be
    found in a Scrabble dictionary, and it doesn't begin or end with a
    stopword."""

    # Remove single-word n-grams that are in the Scrabble dictionary.
    if len(ng) == 1 and ng[0].lower() in scrabble:
        if ng[0] not in ['POS', 'HMM', 'EM', 'PENMAN', 'CHILDES', 'AI']:
            return False

    if len(ng) == 1 and not ng[0][0].isalpha():
        return False

    # Remove n-grams that begin or end with stopwords, e.g., conjunctions,
    # prepositions, or personal pronouns.
    for word in [ng[0], ng[-1]]:
        if word in stop or word.lower() in stop:
            return False
        if word[-1].isdigit() and '-' in word:
            # E.g., 'COLING-92'.
            return False
        if word[-1] == '-' or word[-1] == '.':
            return False

    # Remove n-grams containing words with no ASCII letters, e.g., numbers or
    # symbols.
    for word in ng:
        if not any(c in string.ascii_letters for c in word):
            return False
        if '*' in word:
            return False
    return True


def filter_plurals(ngrams):
    """Remove regular plurals if the list includes the singular."""

    remove = set()
    for ng in ngrams:
        pl = ng[:-1] + (ng[-1] + 's',)
        if pl in ngrams:
            remove.add(pl)
            try:
                # Counts
                ngrams[ng] += ngrams[pl]
            except TypeError:
                # Sets of IDs
                ngrams[ng] |= ngrams[pl]
        pl = ng[:-1] + (ng[-1] + 'es',)
        if pl in ngrams:
            remove.add(pl)
            try:
                # Counts
                ngrams[ng] += ngrams[pl]
            except TypeError:
                # Sets of IDs
                ngrams[ng] |= ngrams[pl]
    for ng in remove:
        del ngrams[ng]
    return ngrams


def filter_subsumed(ngrams):
    """Remove n-grams whose scores are within 25% of subsuming n+1-grams."""

    remove = set()
    for ng in ngrams:
        if len(ng) == 1:
            continue
        shorter = ng[:-1]
        if shorter in ngrams and ngrams[shorter] <= ngrams[ng]*1.3:
            remove.add(shorter)
        shorter = ng[1:]
        if shorter in ngrams and ngrams[shorter] <= ngrams[ng]*1.3:
            remove.add(shorter)
    for ng in remove:
        del ngrams[ng]
    return ngrams


def score_ngrams(ngrams, citations, pc=0.9):
    """Score n-grams based on the density of the citation graph for
    documents containing them."""

    scores = dict()
    for t in ngrams:
        # Build term citation graph.
        gt_nodes = ngrams[t]
        gt_edges = {}
        for node in gt_nodes:
            dests = citations[node].intersection(gt_nodes)
            if dests:
                gt_edges[node] = dests

        # Terms and sets for the equations
        n = float(len(citations))
        na = float(len(gt_nodes))
        nca = float(len(gt_edges.keys()))
        connected_nodes = gt_edges.keys()
        other_nodes = gt_nodes - set(gt_edges.keys())

        # Skip ones where there aren't enough nodes to count.
        if na < 4.0:
            continue

        # Likelihood of observing a tightly connected term citation graph
        # if the term is a topic.
        oh1 = nca * log(pc) + (na - nca) * log(1 - pc)

        # Likelihood of observing a tightly connected term citation graph
        # if the term is not a topic.
        term1 = 0.0
        for i in connected_nodes:
            li = len(citations[i])
            term1 += log(1.0 - (1.0 - (na - 1.0)/(n - 1.0))**li)
        term2 = 0.0
        for i in other_nodes:
            li = len(citations[i])
            term2 += li * log(1.0 - (na - 1.0)/(n - 1.0))
        oh0 = term1 + term2

        scores[t] = oh1 - oh0
    return scores


###


# ScienceDirect API documentation:
#   http://api.elsevier.com/documentation/SCIDIRSearchAPI.wadl


def search_sd(terms):
    """Search ScienceDirect for documents containing specified terms."""

    def chunks(l, n):
        """Yield successive n-sized chunks from l."""
        for i in xrange(0, len(l), n):
            yield l[i:i+n]

    print '-- Search ScienceDirect:', len(terms), 'terms.'

    books = defaultdict(set)
    for result in pool.imap(search_sd_helper, chunks(terms, 200)):
        for book in result:
            books[book] |= result[book]

    return books


def search_sd_helper(terms):
    books = defaultdict(set)

    query = ' OR '.join(['"'+x+'"' for x in terms])

    # Filter chaff in query.
    stop = ['glossary', 'index', 'removed', 'dedication', 'bibliography',
            'copyright', 'front matter', 'frontmatter', 'contributors',
            'acknowledgment', 'acknowledgement', 'preface', 'references',
            'postface', 'edited by', 'list of', 'about the', 'afterword',
            'abbreviations', 'further reading', 'guide to readers',
            'foreword', 'editors', 'author', 'abstract']
    query += ' AND NOT title(' + ' OR '.join(['"'+s+'"' for s in stop]) + ')'

    url = 'http://api.elsevier.com/content/search/index:scidir'
    vals = {'query': query,
            'subscribed': True,
            'oa': True,
            'content': 'nonserial',
            'count': 200,
            'apikey': SD_API_KEY}
    r = requests.get(url, params=vals)

    for entry in r.json()['search-results']['entry']:
        try:
            pii = re.sub('[-().]', '', entry['pii'])
            doc_title = entry['dc:title']
            isbn = entry['prism:isbn']
            pub_title = entry['prism:publicationName']
        except KeyError:
            continue

        books[(isbn, pub_title)].add((pii, doc_title))

    return books


def download_sd(docs_by_book):
    """Download documents from ScienceDirect."""

    doc_ids = set()
    for book in docs_by_book:
        for pii, _ in docs_by_book[book]:
            doc_ids.add(pii)

    print '-- Download ScienceDirect:', len(doc_ids), 'documents.'

    pool.map(download_sd_doc, doc_ids)


def download_sd_doc(pii, view='full'):
    """Download the ScienceDirect document with the specified PII unless
    it has previously been downloaded."""

    file_path = os.path.join(outdir, 'sd-download', pii+'-'+view+'.xml')
    if not os.path.exists(file_path):
        print '   Download:', pii + '-full.xml'

        url = 'http://api.elsevier.com/content/article/pii:' + pii
        vals = {'view': view,
                'apikey': SD_API_KEY}
        r = requests.get(url, params=vals)

        if r.status_code != requests.codes.ok:
            print >> sys.stderr, '!! ScienceDirect server error:', r.status_code
            print >> sys.stderr, r.text
            return

        with codecs.open(file_path, 'w', 'utf8') as out:
            out.write(r.text)


def filter_sd(docs_by_book, terms):
    """Find documents that are highly relevant to the list of terms based
    on the number of occurrences of terms and the number of potentially
    matching documents in the entire book."""

    num_docs = sum([len(docs_by_book[book]) for book in docs_by_book])
    print '-- Filter ScienceDirect:', num_docs, 'documents.'

    books = Counter()
    docs_high = set()
    docs_low = set()

    for book in docs_by_book:
        curried = partial(count_terms_in_doc, terms=terms)
        doc_ids = [pii for (pii, doc_title) in docs_by_book[book]]
        for pii, count, unique in pool.imap(curried, doc_ids):
            if count >= 20 and unique >= 10:
                docs_high.add(pii)
                books[book] += 1
            if count >= 4 and unique >= 2:
                docs_low.add(pii)

    print '-- Potential ScienceDirect:', len(docs_low), 'documents.'

    filtered = set()
    for (book, _) in books.most_common():
        _, book_title = book
        if len(filtered) > .65 * len(docs_high):
            break

        print
        print ' -', book_title.encode('utf8')

        for pii, doc_title in docs_by_book[book]:
            if pii in docs_low:
                print '  ', doc_title.encode('utf8'), pii
                filtered.add(pii)

    print '-- ScienceDirect:', len(filtered), 'documents selected.'

    return filtered


def count_terms_in_doc(pii, terms):
    """Given a ScienceDirect PII and a list of terms, count how many times
    those terms occur in the corresponding document, total and unique."""

    # We can't pass the trie as an argument when using multiprocessing.
    trie = NoAho()
    for term in terms:
        trie.add(term)

    file_path = os.path.join(outdir, 'sd-download', pii + '-full.xml')
    text = codecs.open(file_path, 'r', 'utf8').read().lower()
    matches = [text[x[0]:x[1]] for x in trie.findall_long(text)]

    return [pii, len(matches), len(set(matches))]


def export_sd(ids):
    print '-- Exporting ScienceDirect documents.'

    st = nltk.data.load('tokenizers/punkt/english.pickle')
    abbrevs = ['dr', 'vs', 'mr', 'mrs', 'prof', 'e.g', 'i.e', 'viz', 'cf']
    st._params.abbrev_types.update(abbrevs)

    for pii in ids:
        f = os.path.join(outdir, 'sd-download', pii + '-full.xml')
        xml = codecs.open(f, 'r', 'utf8').read()
        xml = re.sub("([</])(dc|prism|ce|sb|xocs):", r"\1", xml)
        soup = BeautifulSoup(xml)

        fname = 'sd-' + pii.lower() + '.json'

        title = ''
        if soup.title:
            title = soup.title.string.strip()
        book = ''
        if soup.publicationname:
            book = soup.publicationname.string.strip()

        base_url = 'http://www.sciencedirect.com/science/article/pii/'
        j = {'info': {'authors': [x.string.strip() for x in soup('creator')],
                      'id': pii,
                      'title': title,
                      'book': book,
                      'url': base_url + pii},
             'references': [],
             'sections': []}

        if soup.abstract:
            new_sec = {'heading': 'Abstract',
                       'text': st.tokenize(soup.abstract.get_text())}
            j['sections'].append(new_sec)

        for section in soup.find_all('section'):
            new_sec = {'text': []}
            heading = section.find('section-title')
            if heading and heading.string:
                new_sec['heading'] = heading.string.strip()
            for p in section.find_all(['para', 'simple-para']):
                new_sec['text'] += st.tokenize(p.get_text())
            j['sections'].append(new_sec)

        if soup.rawtext and len(j['sections']) < 3:
            j['sections'].append({'text': st.tokenize(soup.rawtext.get_text())})

        with codecs.open(os.path.join(outdir, fname), 'w', 'utf8') as out:
            out.write(json.dumps(j, sort_keys=True, indent=2,
                                 ensure_ascii=False))


###


def search_wiki(terms):
    print '-- Searching Wikipedia:', len(terms), 'terms.'

    titles = set()
    for i, term in enumerate(terms):
        if i % 100 == 0:
            print '   Term', i
        titles.update(set(wiki.search(term, results=2)))

    print '-- Retrieving:', len(titles), 'matches.'

    trie = NoAho()
    for term in terms:
       trie.add(term)

    cats = Counter()
    articles = []

    for title in titles:
        if title.startswith('Category:') or 'List of ' in title or \
           'Index of ' in title:
            continue

        try:
            article = wiki.page(title)
            text = article.content.lower()
        except:
            continue

        if len(text) < 200:
            continue

        articles.append(article)

        # Does it contain at least four terms in the text?
        if len(list(trie.findall_long(text))) > 3:
            for c in article.categories:
                cats[c] += 1

    print '-- Choosing categories.'

    top_cats = set()
    for (cat, c) in cats.most_common(100):
        if cat.startswith('All ') or cat.startswith('Pages ') or \
           cat.startswith('Wikipedia ') or cat.endswith(' stubs') or \
           'articles' in cat.lower() or 'category' in cat.lower() or \
           'CS1' in cat or 'US-centric' in cat or 'USA-centric' in cat or \
           cat == 'Living people' or cat.endswith('Wikidata') or \
           cat.startswith('Use '):
            continue
        print '   Cat:', cat, c
        top_cats.add(cat)

    print '-- Choosing articles.'

    good_articles = set()
    for article in articles:
        try:
            cats = article.categories
            text = article.content.lower()
        except:
            # I don't know why this sometimes fails. Probably it's due to
            # a bad response from Wikipedia.
            continue
        if not top_cats.intersection(set(cats)):
            print '   - Bad cats:', article.title.encode('utf8')
            continue
        if len(list(trie.findall_long(text))) < 3:
            print '   - Not enough terms:', article.title.encode('utf8')
            continue
        print '   + Good:', article.title.encode('utf8')
        good_articles.add(article)

    return good_articles


def export_wiki(articles):
    print '-- Exporting', len(articles), 'articles.'

    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
    abbrevs = ['dr', 'vs', 'mr', 'mrs', 'prof', 'e.g', 'i.e', 'viz', 'cf']
    sentence_tokenizer._params.abbrev_types.update(abbrevs)

    for article in articles:
        article_id = article.title.replace(' ', '_')
        fname = 'wiki-' + re.sub('[:/]', '_', article_id.lower()) + '.json'
        with open(os.path.join(outdir, fname), 'w') as out:
            j = {'info': {'authors': ['Wikipedia'],
                          'id': article_id,
                          'title': article.title,
                          'book': 'Wikipedia',
                          'url': article.url},
                 'references': [x.replace(' ', '_') for x in article.links],
                 'sections': []}

            # As of 2015-11-07, the wikipedia package's parsing of
            # sections is broken. We work around it. Note that for the moment
            # we divide the text into sections and sentences but not by
            # paragraph. I'll probably want to change this later.
            sections = []
            for line in article.content.split('\n'):
                if line.startswith('==') and line.endswith('=='):
                    heading = re.match('=+ (.+)(Edit)? =+$', line).group(1)
                    sections.append({'heading': heading,
                                     'text': []})
                elif len(sections) == 0:
                    sections.append({'text': sentence_tokenizer.tokenize(line,
                                               realign_boundaries=True)})
                else:
                    sections[-1]['text'] += sentence_tokenizer.tokenize(line,
                                               realign_boundaries=True)

            j['sections'] = sections

            out.write(json.dumps(j, sort_keys=True, indent=2,
                                 ensure_ascii=False).encode('utf8'))


###


def search_bing(terms):
    print '-- Searching bing for', len(terms), 'terms.'

    bing = PyBingSearch(BING_API_KEY)

    # The package uses an outdated URL.
    bing.QUERY_URL = 'https://api.datamarket.azure.com/Bing/Search/v1/Web' \
                 + '?Query={}&$top={}&$skip={}&$format={}'

    tutorials = set()
    for term in terms:
        print
        print '-- Search:', term
        query = 'domain:edu  & tutorial & "' + term + '" & filetype:pdf -vitae -cv -syllabus -intitle:references -url:acl'
        results, _ = bing.search(query, limit=10)
        for result in results:
            if re.search('[a-z][0-9][0-9]-[0-9][0-9][0-9][0-9]\.pdf',
                         result.url.lower()):
                # Looks like an ACL paper.
                continue
            if 'acl' in result.url.lower():
                continue
            print ' -', result.title.encode('utf8')
            print '  ', result.url
            tutorials.add(result)
    print
    return tutorials


def download_bing(tutorials):
    print '-- Downloading', len(tutorials), 'tutorials.'

    map(download_tutorial, tutorials)


def download_tutorial(tutorial):
    fname = 'bing-' + re.sub('[ :/()]', '_', tutorial.title[:40].lower())
    fname = fname.replace('_...', '')
    fname = re.sub('_+', '_', fname)
    file_path = os.path.join(outdir, 'bing-download', fname + '.pdf')

    if os.path.exists(file_path):
        print '   Already downloaded:', tutorial.url
    else:
        print '   Download:', tutorial.url
        try:
            r = requests.get(tutorial.url)
            with open(file_path, 'wb') as out:
                out.write(r.content)
        except:
            print ' ! Download failed.'
            if os.path.exists(file_path):
                os.remove(file_path)


def export_bing(tutorials):
    print '-- Exporting tutorials.'

    st = nltk.data.load('tokenizers/punkt/english.pickle')
    abbrevs = ['dr', 'vs', 'mr', 'mrs', 'prof', 'e.g', 'i.e', 'viz', 'cf']
    st._params.abbrev_types.update(abbrevs)

    for tutorial in tutorials:
        fname = 'bing-' + re.sub('[ :/()]', '_', tutorial.title[:40].lower())
        fname = fname.replace('_...', '')
        fname = re.sub('_+', '_', fname)

        if not os.path.exists(os.path.join(outdir, 'bing-download', fname + '.pdf')):
            continue

        with open(os.path.join(outdir, 'bing-download', fname + '.pdf')) as pdf:
            text = ' '.join(slate.PDF(pdf))
        if len(text) < 200:
            continue
        if not ' the ' in text:
            continue
        with open(os.path.join(outdir, fname + '.json'), 'w') as out:
            j = {'info': {'id': fname,
                          'title': tutorial.title,
                          'url': tutorial.url,
                          'authors': '',
                          'book': ''},
                 'sections': [{'text': st.tokenize(text)}]}
            out.write(json.dumps(j, sort_keys=True, indent=2,
                                 ensure_ascii=True))


###


if __name__ == '__main__':
    if len(sys.argv) != 3:
        print >> sys.stderr, 'Usage: build-corpus [corpus] [output]'
        sys.exit(1)

    corpus, outdir = sys.argv[1:]

    pool = mp.Pool(PROCESSES)

    terms = get_terms(corpus)

#    docs_by_book = search_sd(terms)
#    download_sd(docs_by_book)
#    doc_ids = filter_sd(docs_by_book, terms)
#    export_sd(doc_ids)

    articles = search_wiki(terms[:1000])
#    export_wiki(articles)

#    tutorials = search_bing(terms[:100])
#    download_bing(tutorials)
#    export_bing(tutorials)
