#!/usr/bin/env python
# -*- coding: utf-8 -*-

# T: Build Corpus
# Jonathan Gordon

from __future__ import unicode_literals

import sys
import os
import io
import string
import re
import json
import requests
import nltk
import slate
import multiprocessing as mp

from math import log
from functools import partial
from collections import defaultdict, Counter
from noaho import NoAho
from bs4 import BeautifulSoup

import wikipedia as wiki
from py_bing_search import PyBingSearch

from t.corpus import Corpus, Document
from t.lx import ScrabbleLexicon, StopLexicon


# Parameters

PROCESSES = int(.5 * mp.cpu_count())

SD_API_KEY = open(os.path.expanduser('~/.t/sd.txt')).read().strip()
BING_API_KEY = open(os.path.expanduser('~/.t/bing.txt')).read().strip()


###


def get_terms(dirname):
    """Return an ordered list of technical terms or names of research
    topics, based on citation graph density."""

    stop = StopLexicon()
    scrabble = ScrabbleLexicon()

    print '-- Reading corpus.'
    ngrams = defaultdict(set)
    citations = defaultdict(set)
    for i, docname in enumerate(os.listdir(dirname)):
        if i % 2000 == 0:
            print '   Document', i
        with io.open(os.path.join(dirname, docname), 'r', encoding='utf8') as f:
            j = json.load(f)
            doc_id = j['info']['id']
            if not 'title' in j['info']:
                print '   No title for', docname
                continue
            for ng in get_ngrams([j['info']['title']]):
                if not good_ngram(ng, scrabble, stop):
                    continue
                ngrams[ng].add(doc_id)
            for ref in j['references']:
                citations[doc_id].add(ref['id'])
                citations[ref['id']].add(doc_id)

    ngrams = filter_plurals(ngrams)

    ngram_counts = dict([(x, len(ngrams[x])) for x in ngrams])
    filtered = filter_subsumed(ngram_counts)

    ngrams = score_ngrams(ngrams, citations)
    ngrams = filter_subsumed(ngrams)

    return [' '.join(x) for x in sorted(ngrams, key=lambda x: ngrams[x],
                                        reverse=True) if x in filtered]


def get_abstracts(dirname, maxdocs=None):
    """Return a list of abstracts from the JSON documents in the corpus."""

    def get_abstract(j):
        """Return the abstract of the JSON document as a list of sentences. An
        abstract is assumed to be the section with the heading 'Abstract' or,
        if there isn't one, the first section."""

        for section in j['sections'][:3]:
            if 'heading' not in section or not section['heading']:
                continue
            if section['heading'].lower() == 'abstract':
                return section['text'][:25]
        if len(j['sections']) > 0:
            return j['sections'][0]['text'][:25]

        print '   No sections in', docname
        return []

    abstracts = []
    for i, docname in enumerate(os.listdir(dirname)):
        if i % 2000 == 0:
            print '   Document', i
        if maxdocs and i >= maxdocs:
            break
        with io.open(os.path.join(dirname, docname), 'r', encoding='utf8') as f:
            abstr = get_abstract(json.load(f))
            abstracts.append(abstr)
    return abstracts



def get_ngrams(l, min=1):
    """Return the set of all n-grams that occur in the input strings, at
    least 'min' times."""

    def everygrams(seq):
        """Return all possible n-grams generated from a sequence of items,
        as an iterator."""
        for n in range(1, len(seq) + 1):
            for ng in nltk.util.ngrams(seq, n):
                yield ng

    counts = defaultdict(int)
    for i, s in enumerate(l):
        if i % 2000 == 0 and i > 0:
            print '   Item', i

        tokens = nltk.tokenize.word_tokenize(s)

        # Preserve case for acronyms inside n-grams but lowercase everything
        # else.
        new_tokens = []
        for token in tokens:
            if token.isupper() and len(token) > 1:
                new_tokens.append(token)
            else:
                new_tokens.append(token.lower())
        tokens = new_tokens

        for ngram in everygrams(tokens):
            counts[ngram] += 1

    return dict([(x, counts[x]) for x in counts if counts[x] >= min])


def good_ngram(ng, scrabble, stop):
    """Check if the n-gram is good: It's not a single word that would be
    found in a Scrabble dictionary, and it doesn't begin or end with a
    stopword."""

    # Remove single-word n-grams that are in the Scrabble dictionary.
    if len(ng) == 1 and ng[0].lower() in scrabble:
        if ng[0] not in ['POS', 'HMM', 'EM', 'PENMAN', 'CHILDES', 'AI']:
            return False

    if len(ng) == 1 and not ng[0][0].isalpha():
        return False

    # Remove n-grams that begin or end with stopwords, e.g., conjunctions,
    # prepositions, or personal pronouns.
    for word in [ng[0], ng[-1]]:
        if word in stop or word.lower() in stop:
            return False
        if word[-1].isdigit() and '-' in word:
            # E.g., 'COLING-92'.
            return False
        if word[-1] == '-' or word[-1] == '.':
            return False

    # Remove n-grams containing words with no ASCII letters, e.g., numbers or
    # symbols.
    for word in ng:
        if not any(c in string.ascii_letters for c in word):
            return False
        if '*' in word:
            return False
    return True


def filter_plurals(ngrams):
    """Remove regular plurals if the list includes the singular."""

    remove = set()
    for ng in ngrams:
        pl = ng[:-1] + (ng[-1] + 's',)
        if pl in ngrams:
            remove.add(pl)
            try:
                # Counts
                ngrams[ng] += ngrams[pl]
            except TypeError:
                # Sets of IDs
                ngrams[ng] |= ngrams[pl]
        pl = ng[:-1] + (ng[-1] + 'es',)
        if pl in ngrams:
            remove.add(pl)
            try:
                # Counts
                ngrams[ng] += ngrams[pl]
            except TypeError:
                # Sets of IDs
                ngrams[ng] |= ngrams[pl]
    for ng in remove:
        del ngrams[ng]
    return ngrams


def filter_subsumed(ngrams):
    """Remove n-grams whose scores are within 25% of subsuming n+1-grams."""

    remove = set()
    for ng in ngrams:
        if len(ng) == 1:
            continue
        shorter = ng[:-1]
        if shorter in ngrams and ngrams[shorter] <= ngrams[ng]*1.3:
            remove.add(shorter)
        shorter = ng[1:]
        if shorter in ngrams and ngrams[shorter] <= ngrams[ng]*1.3:
            remove.add(shorter)
    for ng in remove:
        del ngrams[ng]
    return ngrams


def score_ngrams(ngrams, citations, pc=0.9):
    """Score n-grams based on the density of the citation graph for
    documents containing them."""

    scores = dict()
    for t in ngrams:
        # Build term citation graph.
        gt_nodes = ngrams[t]
        gt_edges = {}
        for node in gt_nodes:
            dests = citations[node].intersection(gt_nodes)
            if dests:
                gt_edges[node] = dests

        # Terms and sets for the equations
        n = float(len(citations))
        na = float(len(gt_nodes))
        nca = float(len(gt_edges.keys()))
        connected_nodes = gt_edges.keys()
        other_nodes = gt_nodes - set(gt_edges.keys())

        # Skip ones where there aren't enough nodes to count.
        if na < 4.0:
            continue

        # Likelihood of observing a tightly connected term citation graph
        # if the term is a topic.
        oh1 = nca * log(pc) + (na - nca) * log(1 - pc)

        # Likelihood of observing a tightly connected term citation graph
        # if the term is not a topic.
        term1 = 0.0
        for i in connected_nodes:
            li = len(citations[i])
            term1 += log(1.0 - (1.0 - (na - 1.0)/(n - 1.0))**li)
        term2 = 0.0
        for i in other_nodes:
            li = len(citations[i])
            term2 += li * log(1.0 - (na - 1.0)/(n - 1.0))
        oh0 = term1 + term2

        scores[t] = oh1 - oh0
    return scores


###


# ScienceDirect API documentation:
#   http://api.elsevier.com/documentation/SCIDIRSearchAPI.wadl


def search_sd(terms):
    """Search ScienceDirect for documents containing specified terms."""

    def chunks(l, n):
        """Yield successive n-sized chunks from l."""
        for i in xrange(0, len(l), n):
            yield l[i:i+n]

    print '-- Search ScienceDirect:', len(terms), 'terms.'

    books = defaultdict(set)
    for result in pool.imap(search_sd_helper, chunks(terms, 200)):
        for book in result:
            books[book] |= result[book]

    return books


def search_sd_helper(terms):
    books = defaultdict(set)

    query = ' OR '.join(['"'+x+'"' for x in terms])

    # Filter chaff in query.
    stop = ['glossary', 'index', 'removed', 'dedication', 'bibliography',
            'copyright', 'front matter', 'frontmatter', 'contributors',
            'acknowledgment', 'acknowledgement', 'preface', 'references',
            'postface', 'edited by', 'list of', 'about the', 'afterword',
            'abbreviations', 'further reading', 'guide to readers',
            'foreword', 'editors', 'author', 'abstract']
    query += ' AND NOT title(' + ' OR '.join(['"'+s+'"' for s in stop]) + ')'

    url = 'http://api.elsevier.com/content/search/index:scidir'
    vals = {'query': query,
            'subscribed': True,
            'oa': True,
            'content': 'nonserial',
            'count': 200,
            'apikey': SD_API_KEY}
    r = requests.get(url, params=vals)

    for entry in r.json()['search-results']['entry']:
        try:
            pii = re.sub('[-().]', '', entry['pii'])
            doc_title = entry['dc:title']
            isbn = entry['prism:isbn']
            pub_title = entry['prism:publicationName']
        except KeyError:
            continue

        books[(isbn, pub_title)].add((pii, doc_title))

    return books


def download_sd(docs_by_book):
    """Download documents from ScienceDirect."""

    doc_ids = set()
    for book in docs_by_book:
        for pii, _ in docs_by_book[book]:
            doc_ids.add(pii)

    print '-- Download ScienceDirect:', len(doc_ids), 'documents.'

    pool.map(download_sd_doc, doc_ids)


def download_sd_doc(pii, view='full'):
    """Download the ScienceDirect document with the specified PII unless
    it has previously been downloaded."""

    file_path = os.path.join(outdir, 'sd-download', pii+'-'+view+'.xml')
    if not os.path.exists(file_path):
        print '   Download:', pii + '-full.xml'

        url = 'http://api.elsevier.com/content/article/pii:' + pii
        vals = {'view': view,
                'apikey': SD_API_KEY}
        r = requests.get(url, params=vals)

        if r.status_code != requests.codes.ok:
            print >> sys.stderr, '!! ScienceDirect server error:', r.status_code
            print >> sys.stderr, r.text
            return

        with io.open(file_path, 'w', encoding='utf8') as out:
            out.write(r.text)


def filter_sd(docs_by_book, terms):
    """Find documents that are highly relevant to the list of terms based
    on the number of occurrences of terms and the number of potentially
    matching documents in the entire book."""

    num_docs = sum([len(docs_by_book[book]) for book in docs_by_book])
    print '-- Filter ScienceDirect:', num_docs, 'documents.'

    books = Counter()
    docs_high = set()
    docs_low = set()

    for book in docs_by_book:
        curried = partial(count_terms_in_doc, terms=terms)
        doc_ids = [pii for (pii, doc_title) in docs_by_book[book]]
        for pii, count, unique in pool.imap(curried, doc_ids):
            if count >= 20 and unique >= 10:
                docs_high.add(pii)
                books[book] += 1
            if count >= 4 and unique >= 2:
                docs_low.add(pii)

    print '-- Potential ScienceDirect:', len(docs_low), 'documents.'

    filtered = set()
    for (book, _) in books.most_common():
        _, book_title = book
        if len(filtered) > .65 * len(docs_high):
            break

        print
        print ' -', book_title.encode('utf8')

        for pii, doc_title in docs_by_book[book]:
            if pii in docs_low:
                print '  ', doc_title.encode('utf8'), pii
                filtered.add(pii)

    print '-- ScienceDirect:', len(filtered), 'documents selected.'

    return filtered


def count_terms_in_doc(pii, terms):
    """Given a ScienceDirect PII and a list of terms, count how many times
    those terms occur in the corresponding document, total and unique."""

    # We can't pass the trie as an argument when using multiprocessing.
    trie = NoAho()
    for term in terms:
        trie.add(term)

    file_path = os.path.join(outdir, 'sd-download', pii + '-full.xml')
    text = io.open(file_path, 'r', encoding='utf8').read().lower()
    matches = [text[x[0]:x[1]] for x in trie.findall_long(text)]

    return [pii, len(matches), len(set(matches))]


def export_sd(ids):
    print '-- Exporting ScienceDirect documents.'

    st = nltk.data.load('tokenizers/punkt/english.pickle')
    abbrevs = ['dr', 'vs', 'mr', 'mrs', 'prof', 'e.g', 'i.e', 'viz', 'cf',
               'proc', 'b']
    st._params.abbrev_types.update(abbrevs)

    c = Corpus()

    for pii in ids:
        f = os.path.join(outdir, 'sd-download', pii + '-full.xml')
        xml = io.open(f, 'r', encoding='utf8').read()
        xml = re.sub("([</])(dc|prism|ce|sb|xocs):", r"\1", xml)
        soup = BeautifulSoup(xml)

        d = Document()
        d.id = 'sd-' + pii.lower()
        d.authors = [x.string.strip() for x in soup('creator')]
        if soup.title:
            d.title = soup.title.string.strip()
        if soup.publicationname:
            d.book = soup.publicationname.string.strip()
        d.url = 'http://www.sciencedirect.com/science/article/pii/' + pii
        d.authors = [x.string.strip() for x in soup('creator')]

        if soup.abstract:
            sec = {'heading': 'Abstract',
                   'text': st.tokenize(soup.find('abstract-sec').get_text())}
            d.sections.append(sec)

        for section in soup.find_all('section'):
            sec = {'text': []}
            heading = section.find('section-title')
            if heading and heading.string:
                sec['heading'] = heading.string.strip()
            for p in section.find_all(['para', 'simple-para']):
                sec['text'] += st.tokenize(p.get_text())
            d.sections.append(sec)

        if len(d.sections) == 0:
            sec = {'text': []}
            for p in section.find_all(['para', 'simple-para']):
                sec['text'] += st.tokenize(p.get_text())
            d.sections.append(sec)

        if soup.rawtext and len(d.sections) < 3:
            d.sections.append({'text': st.tokenize(soup.rawtext.get_text())})

        c.add(d)

    c.export(outdir)


###


def search_wiki(terms):
    """Search Wikipedia for articles containing specified terms."""

    print '-- Search Wikipedia:', len(terms), 'terms.'

    titles = set()
    for result in pool.imap(search_wiki_helper, terms):
        titles.update(result)

    print '-- Download Wikipedia:', len(titles), 'articles.'

    categories = defaultdict(set)
    docs_high = []
    docs_low = []

    curried = partial(get_wiki_article, terms=terms)
    for article, count, unique in pool.imap(curried, titles):
        if article:
            if count >= 15 and unique >= 5:
                docs_high.append(article)
                try:
                    for c in article.categories:
                        categories[c].add(article.title)
                except:
                    pass
            if count >= 4 and unique >= 2:
                docs_low.append(article)
            else:
                print '   - Not enough terms:', article.title.encode('utf8')

    print '-- Choosing categories.'

    covered = set()
    top_categories = set()
    for cat in sorted(categories, key=lambda x: len(categories[x]),
                      reverse=True):
        if len(covered) >= .6 * len(docs_high):
            break
        if len(categories[cat]) == 1:
            break
        if cat.startswith('All ') or cat.startswith('Pages ') or \
           'Wikipedia' in cat or cat.endswith(' stubs') or \
           'articles' in cat.lower() or 'category' in cat.lower() or \
           'CS1' in cat or 'US-centric' in cat or 'USA-centric' in cat or \
           'iving people' in cat or 'Wikidata' in cat or \
           cat.startswith('Use ') or cat.endswith(' alumni') or \
           'University' in cat or cat.endswith(' deaths') or \
           cat.endswith(' births') or 'ompanies' in cat or \
           'EngvarB' in cat or 'ambiguous time' in cat or \
           'needing confirmation' in cat:
            continue
        if len(top_categories) > .65 * len(categories):
            break
        print '  - Selected category:', cat.encode('utf8'), len(categories[cat])
        top_categories.add(cat)
        covered |= categories[cat]

    print '-- Choosing articles.'

    good_articles = set()
    for article in docs_low:
        try:
            categories = article.categories
            text = article.content.lower()
        except:
            continue

        if not top_categories.intersection(set(categories)):
            print '   - Bad categories:', article.title.encode('utf8')
            continue
        print '   + Good:', article.title.encode('utf8')
        good_articles.add(article)

    return good_articles


def search_wiki_helper(term):
    return set(wiki.search(term, results=2))


def get_wiki_article(title, terms):
    # We can't pass the trie as an argument when using multiprocessing.
    trie = NoAho()
    for term in terms:
       trie.add(term)

    if title.startswith('Category:') or 'List of ' in title or 'Index of ' \
       in title:
        return (None, None, None)

    try:
        article = wiki.page(title)
        text = article.content.lower()
    except:
        return (None, None, None)

    if len(text) < 200:
        return (None, None, None)

    matches = [text[x[0]:x[1]] for x in trie.findall_long(text)]

    return (article, len(matches), len(set(matches)))


def export_wiki(articles):
    print '-- Exporting', len(articles), 'articles.'

    pool.map(export_wiki_article, articles)


def export_wiki_article(article):
    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
    abbrevs = ['dr', 'vs', 'mr', 'mrs', 'prof', 'e.g', 'i.e', 'viz', 'cf',
               'proc', 'b']
    sentence_tokenizer._params.abbrev_types.update(abbrevs)

    d = Document()
    d.id = 'wiki-' + re.sub('[:/ ]', '_', article.title.lower())
    d.authors = ['Wikipedia']
    d.title = article.title
    d.book = 'Wikipedia'

    try:
        d.url = article.url
        d.references = ['wiki-' + re.sub('[:/ ]', '_', x.lower())
                        for x in article.links]
    except:
        return

    # As of 2015-11-07, the wikipedia package's parsing of
    # sections is broken. We work around it. Note that for the moment
    # we divide the text into sections and sentences but not by
    # paragraph. I'll probably want to change this later.
    for line in article.content.split('\n'):
        if line.startswith('==') and line.endswith('=='):
            line = line.replace('Edit =', ' =')
            heading = re.match('=+ (.+) =+$', line).group(1)
            d.sections.append({'heading': heading,
                               'text': []})
        elif len(d.sections) == 0:
            d.sections.append({'text': sentence_tokenizer.tokenize(line,
                                          realign_boundaries=True)})
        else:
            d.sections[-1]['text'] += sentence_tokenizer.tokenize(line,
                                           realign_boundaries=True)

    with io.open(os.path.join(outdir, d.id + '.json'), 'w',
                 encoding='utf8') as out:
        out.write(d.json())


###


def search_bing(terms):
    print '-- Searching bing for', len(terms), 'terms.'

    tutorials = Counter()
    for results in pool.imap(search_bing_term, terms):
        for result in results:
            title, url = result
            tutorials[result] += 1

    multimatch = set([x for x in tutorials if tutorials[x] > 1])
    print '--', len(multimatch), 'multiple matches.'

    by_site = defaultdict(set)
    for (title, url) in multimatch:
        site = re.sub('^https?://(www.)?', '', url)
        site = re.sub('\.edu/.+', '.edu', site)
        by_site[site].add((title, url))
    print '--', len(by_site), 'sites.'

    choices = set()
    for site in sorted(by_site, key=lambda x: len(by_site[x]), reverse=True):
        if len(choices) >= .65 * len(multimatch):
            break
        choices |= by_site[site]
        for result in by_site[site]:
            print ' -', title.encode('utf8')
            print '  ', url.encode('utf8')[:70]

    print '--', len(choices), 'choices.'

    return choices


def search_bing_term(term):
    tutorials = set()
    try:
        bing = PyBingSearch(BING_API_KEY)

        # The package uses an outdated URL.
        bing.QUERY_URL = 'https://api.datamarket.azure.com/Bing/Search/v1/Web' \
                     + '?Query={}&$top={}&$skip={}&$format={}'

        query = 'domain:edu & tutorial & "' + term.encode('utf8') + '" & filetype:pdf -vitae -cv -syllabus -intitle:references -url:acl -resume -programme -dissertation -thesis -phd'
        results, _ = bing.search(query, limit=10)
        for result in results:
            if re.search('[a-z][0-9][0-9]-[0-9][0-9][0-9][0-9]\.pdf',
                         result.url.lower()):
                # Looks like an ACL paper.
                continue
            if 'acl' in result.url.lower():
                continue
            tutorials.add((result.title, result.url))
    except:
        print >> sys.stderr, 'Unexpected error:', sys.exc_info()[0]
        pass

    return tutorials


def download_bing(tutorials):
    print '-- Downloading', len(tutorials), 'tutorials.'

    pool.map(download_tutorial, tutorials)


def download_tutorial(tutorial):
    title, url = tutorial

    fname = 'bing-' + re.sub('[ :/()]', '_', title[:40].lower())
    fname = fname.replace('_...', '')
    fname = re.sub('_+', '_', fname)
    file_path = os.path.join(outdir, 'bing-download', fname + '.pdf')

    if not os.path.exists(file_path):
        try:
            r = requests.get(url)
            with open(file_path, 'wb') as out:
                out.write(r.content)
        except:
            print >> sys.stderr, ' ! Download failed.'
            if os.path.exists(file_path):
                os.remove(file_path)


def export_bing(tutorials):
    print '-- Exporting', len(tutorials), 'tutorials.'
    pool.map(export_tutorial, tutorials)


def export_tutorial(tutorial):
    title, url = tutorial

    fname = 'bing-' + re.sub('[ :/()]', '_', title[:40].lower())
    fname = fname.replace('_...', '')
    fname = re.sub('_+', '_', fname)

    if not os.path.exists(os.path.join(outdir, 'bing-download', fname+'.pdf')):
        print >> sys.stderr, ' ! Missing download:', fname.encode('utf8')
        return

    st = nltk.data.load('tokenizers/punkt/english.pickle')
    abbrevs = ['dr', 'vs', 'mr', 'mrs', 'prof', 'e.g', 'i.e', 'viz', 'cf',
               'proc', 'b']
    st._params.abbrev_types.update(abbrevs)

    with open(os.path.join(outdir, 'bing-download', fname + '.pdf')) as pdf:
        try:
            text = ' '.join(slate.PDF(pdf)).decode('utf8')
        except:
            return

    if len(text) < 400 or not ' the ' in text:
        return

    d = Document()
    d.id = fname
    d.title = title
    d.url = url
    d.authors = []
    d.book = ''
    d.sections = [{'text': st.tokenize(text)}]

    with io.open(os.path.join(outdir, fname + '.json'), 'w',
                 encoding='utf8') as out:
        out.write(d.json)


###


if __name__ == '__main__':
    if len(sys.argv) != 3:
        print >> sys.stderr, 'Usage: build-corpus [corpus] [output]'
        sys.exit(1)

    corpus, outdir = sys.argv[1:]

    pool = mp.Pool(PROCESSES)

    terms = get_terms(corpus)

    docs_by_book = search_sd(terms)
    download_sd(docs_by_book)
    doc_ids = filter_sd(docs_by_book, terms)
    export_sd(doc_ids)

    articles = search_wiki(terms)
    export_wiki(articles)

    tutorials = search_bing(terms[:5])
    download_bing(tutorials)
    export_bing(tutorials
