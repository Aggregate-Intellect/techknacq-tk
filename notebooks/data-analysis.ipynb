{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Case Study: Generating Binding Affinities for PPI by Reading Biomedical Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The goal for this case study is to identify binding PPIs in a set of papers, normalize the protein names to the UNIPROT ontology IDs, find the associated affinity value and create a table with this information so as to facilitate biomedical simulations. The best gold standard we currently have to evaluate our performance is the INTACT database was manually curated from the papers (which are available in a variety of forms—scanned images, searchable PDFs and nXML files). We will only process the ones that are available in the nXML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The pipeline comprises the following pieces:\n",
    "\n",
    "1. Sentence extracted from PUBMED articles (Ulf)\n",
    "1. AMR parsing (Michael)\n",
    "1. AMR grounding (Jose-Luis)\n",
    "1. Interaction classifier (Sahil)\n",
    "1. Affinity extractor (Sameer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Description of various files in the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "This is a brief description of the files in this folder.\n",
    "\n",
    "- `corpora/intact---korkut/intact-binding-affinities.txt`—A cleaned version of the INTACT database.\n",
    "- `kb-id-distribution.txt`—This is the distribution of various KB ids that are used in the intact database.\n",
    "- `pmids-to-pmcids-mapping---intact.csv`—This is the mapping retrieed from PubMed using their REST API\n",
    "- `pmids-with-pmcids---intact.csv`—A subset of the previous that has mapping information\n",
    "- `pmids-without-pmcids---intact.csv`—A subset of the previous representation, but which do not have corresponding pmids\n",
    "- `1000-papers-from-2015-evaluation---sahil`—Trial output of 1000 papers from June 2015 evaluation. Took approx. 9 hours to process the papers.\n",
    "- `index_cards/`—This directory contains the index cards for the documents inside `parag-2017`. An index card is a json file with one object per file.\n",
    "- `metadata/uniprot-all-human-id-map.sed`—Mapping between the IDs produced by Luis's grounding system nto the ones that are used by the INTACT database.\n",
    "\n",
    "<!--\n",
    "  - eval-auto-amr.amr\n",
    "    amr file.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Data Organization and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Collect various layers in one place by document ID of PMID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
<<<<<<< HEAD
    "hidden": true,
=======
>>>>>>> updated various data files in one swoop
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Collect index cards from `inbox/sahil/index_cards` into `data/intact` after normalizing the IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
<<<<<<< HEAD
    "collapsed": true,
=======
    "collapsed": false,
>>>>>>> updated various data files in one swoop
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "BMDIR=$HOME/workspace/isi---big-mechanism---binding-affinity\n",
    "\n",
    "source $BMDIR/scripts/big-mechanism-functions.bash\n",
    "\n",
    "bm-collect-index-cards"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
=======
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Some of the papers do not have `index_cards` extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
>>>>>>> updated various data files in one swoop
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "FIXME: We will create empty placeholders for them for latter script that expects a `index_cards` sub-directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for d in ../data/intact/*; do \n",
    "    if [[ ! -d $d/index_cards ]]; then\n",
    "        mkdir $d/index_cards\n",
    "    else\n",
    "        echo skpping $d...\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dline ()\n",
    "{    \n",
    "    printf \"%100s\" \"-\" | sed 's/ /-/g'; echo\n",
    "}\n",
    "\n",
    "# Create the (missing) document level intact.txt files\n",
    "for d in ../data/intact/pmid_*; do \n",
    "    # The intact database has yet another variation of the description `pubmed:` instead of `pmid_`\n",
    "    pubmed_id=${d##*/}\n",
    "    pubmed_id=${pubmed_id/pmid_/pubmed:}\n",
    "    \n",
    "    grep $pubmed_id ../corpora/intact---korkut/intact-binding-affinities.txt > $d/intact.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Normalize the IDs inside all data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "BMDIR=$HOME/workspace/isi---big-mechanism---binding-affinity\n",
    "source $BMDIR/scripts/big-mechanism-functions.bash\n",
    "bm-normalize-ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Create a list of `pmid`s for which we have full documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "BMDIR=$HOME/workspace/isi---big-mechanism---binding-affinity\n",
    "source $BMDIR/scripts/big-mechanism-functions.bash\n",
    "\n",
    "bm-create-list-of-pmids-with-full-documents ()\n",
    "\n",
    "head $BMDIR/reports/pmids-with-full-documents.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Create a `pmid2pmcid` mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'pmid_21581186': u'pmc_2959986',\n",
      " u'pmid_21581188': u'pmc_2960142',\n",
      " u'pmid_21581189': u'pmc_2960117',\n",
      " u'pmid_22399676': u'pmc_3338010',\n",
      " u'pmid_22399678': u'pmc_3324521',\n",
      " u'pmid_24718087': u'pmc_3981762',\n",
      " u'pmid_24718089': u'pmc_3981863',\n",
      " u'pmid_24813251': u'pmc_4057660',\n",
      " u'pmid_24813252': u'pmc_4172922',\n",
      " u'pmid_6096475': u'pmc_2187529'}\n"
     ]
    }
   ],
   "source": [
    "import re, codecs, sys\n",
    "from pprint import pprint\n",
    "\n",
    "from l2k2r2.util import initialize_pmid2pmcid_map\n",
    "from l2k2r2.util import Mappings\n",
    "from l2k2r2.util import peekdict\n",
    "\n",
    "\n",
    "initialize_pmid2pmcid_map(\"../metadata/mapping---pmc-to-pmid.csv\")\n",
    "pprint(peekdict(Mappings.p2p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Create human readable to other uniprotkb mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [],
<<<<<<< HEAD
    "collapsed": false,
=======
    "collapsed": true,
>>>>>>> updated various data files in one swoop
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uniprot:1a26_human': 'uniprotkb:p30450',\n",
      " 'uniprot:a0a059qm38_human': 'uniprotkb:a0a059qm38',\n",
      " 'uniprot:cnot2_human': 'uniprotkb:q9nzn8',\n",
      " 'uniprot:f5h6q7_human': 'uniprotkb:f5h6q7',\n",
      " 'uniprot:f6ru00_human': 'uniprotkb:f6ru00',\n",
      " 'uniprot:h0yee5_human': 'uniprotkb:h0yee5',\n",
      " 'uniprot:h6v7t0_human': 'uniprotkb:h6v7t0',\n",
      " 'uniprot:k7erd3_human': 'uniprotkb:k7erd3',\n",
      " 'uniprot:l7rda5_human': 'uniprotkb:l7rda5',\n",
      " 'uniprot:q9uqr5_human': 'uniprotkb:q9uqr5'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from l2k2r2.util import initialize_human2uniprotid_map\n",
    "\n",
    "initialize_human2uniprotid_map(\"../metadata/uniprot-all-human-id-map.tsv\")\n",
    "pprint(peekdict(Mappings.h2u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Split the one big `.amr` file into per document `.amr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
    "hidden": true,
=======
    "heading_collapsed": true,
>>>>>>> updated various data files in one swoop
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Distribution of number of affinity values per document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
    "code_folding": [],
    "collapsed": false,
=======
>>>>>>> updated various data files in one swoop
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Create a `id2affinities` map to store the affinity values for a given PUBMED ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "code_folding": [],
    "collapsed": true,
=======
    "code_folding": [
     0
    ],
    "collapsed": false,
>>>>>>> updated various data files in one swoop
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_of_affinity_values_in_documents():\n",
    "    import csvkit\n",
    "    from collections import defaultdict\n",
    "    id2affinities = defaultdict(list)\n",
    "\n",
    "    reader = csvkit.reader(open(\"../corpora/intact---korkut/gold-standard-intact-binding-affinities.csv\"))\n",
    "    for row in reader:\n",
    "        pmid=row[12].replace(\"pubmed:\", \"pmid_\")\n",
    "        kd=row[9]\n",
    "        type=row[0]\n",
    "        a=row[2]\n",
    "        b=row[5]\n",
    "        id2affinities[pmid].append([type, a, b, kd])\n",
    "\n",
    "    pmids_with_full_documents = [x.strip() for x in open(\"../reports/pmids-with-full-documents.txt\").readlines()]\n",
    "    count = 0\n",
    "    for pmid in sorted(id2affinities):\n",
    "        affinities = id2affinities[pmid]\n",
    "        num_affinities = len(affinities)\n",
    "\n",
    "\n",
    "        if pmid in pmids_with_full_documents and pmid in Mappings.p2p and num_affinities < 100:\n",
    "            pprint([pmid, Mappings.p2p[pmid], num_affinities, affinities])\n",
    "            count = count + 1\n",
    "    print count\n",
    "\n",
    "\n",
    "    # TODO: Plot a histogram of number of affinities in a document\n",
    "plot_histogram_of_affinity_values_in_documents()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "heading_collapsed": true,
=======
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
>>>>>>> updated various data files in one swoop
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "bm-generate-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for code to split .amr file into individual document-level files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
    "heading_collapsed": true,
=======
    "hidden": true,
>>>>>>> updated various data files in one swoop
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<!--\n",
    "### pmid_ ()\n",
    "#### Information in INTACT for this document (via intact.txt)\n",
    "\n",
    "<pre>\n",
    "</pre>\n",
    "\n",
    "\n",
    "#### Pattern match over document (.nxml)\n",
    "\n",
    "<pre>\n",
    "</pre>\n",
    "\n",
    "#### Potential sentence context (from sentences.txt) within reach +/-3 sentences of evidence sentence(s) in index_cards (.json)\n",
    "<pre>\n",
    "</pre>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
    "hidden": true,
=======
    "heading_collapsed": true,
>>>>>>> updated various data files in one swoop
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Data Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
    "code_folding": [
     0
    ],
    "collapsed": false,
=======
    "heading_collapsed": true,
>>>>>>> updated various data files in one swoop
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### High Priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
    "code_folding": [
     0
    ],
    "collapsed": true,
=======
    "heading_collapsed": true,
>>>>>>> updated various data files in one swoop
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Low Priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
    "heading_collapsed": true,
=======
    "hidden": true,
>>>>>>> updated various data files in one swoop
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- **PMID inside index card PMC field**—The index cards currently indicate a `pmid` in a `pmc` field. It should not affect us in any significant way, but should be kept in mind.\n",
    "\n",
    "- **Many ID variants**—Not sure why we are using so many variations on the IDs. The folder name is `pmid_00000000`. The `sentences.txt` uses `a2_pmid_0000_0000`. The latter is also used in the index cards. Is there a particular reason to reformat the number using an underscore. Seems to be an underscore four digits from the right. So some IDs have the pattern `\\d{3}_\\d{4}` and some are `\\d{4}_\\d{4}`. It might make sense to use the same ID format everywhere. Sentences are marked with a `.<number>` suffix to the IDs.\n",
    "\n",
    "- **Icon\\* files**—There are various `Icon*` files—presumably a relic of Dropbox or some other cloud service. Ideally we should not generate them.\n",
    "\n",
    "- **Non-standard characters in filenames**—We should avoid having spaces and non-allowed punctuation such as  `(` in the filenames.\n",
    "\n",
    "- **Some sentences missing AMRs**—Out of 27829 sentences, we have AMRs for 27794. Might want to check why the 30 odd sentences did not generate an AMR. Assuming every sentence should always generate an AMR.\n",
    "\n",
    "- **Extra Colon**—In the normalized data there seem to be extra colon. There should not be any. Refer to Jose-Luis's email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
<<<<<<< HEAD
    "code_folding": [
     0
    ],
    "collapsed": true,
=======
>>>>>>> updated various data files in one swoop
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- **Spurious newlines**—There seem to be a lot of newline characters (`\\n`) in the evidence sentences. These do not appear in the original sentences.txt file. Find out whether these were introduced during AMR parsing or during interaction classification\n",
    "- **Encoding**—There are unicode character references (most likely greek characters, such as delta) in the evidence sentences inside the index card `.json` files. We should ensure that the encoding matches across the AMR and other training data.\n",
    "- **HTML Entities in the PubMed Articles**—Only a very small fraction of the 148 unique HTML entities in the biomedical articles have a reasonable mapping in ASCII land—using the ``unicodedata`` module in Python. <mark>It would be good to normalize them using a mapping table</mark>."
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cases for various patterns in text\n",
    "# These are identified\n",
    "print check_match(plain_affinity, \" 56 nM \")\n",
    "print check_match(plain_affinity, \" (56 nM) \")\n",
    "print check_match(plain_affinity, \" 56.0 nM \")\n",
    "print check_match(plain_affinity, \" 5.6 nM \")\n",
    "print check_match(plain_affinity, \" .56 nM \")\n",
    "print check_match(plain_affinity, \"-56 nM \")\n",
    "print check_match(plain_affinity, \"56,\")\n",
    "print check_match(plain_affinity, \"+5.6 nM \")\n",
    "print check_match(plain_affinity, \"\\n.56 nM \")\n",
    "\n",
    "# The following are deliberately filtered out\n",
    "print check_match(plain_affinity, \">56<nM \")\n",
    "\n",
    "# The following cases are not captured (because of the addition of 0)\n",
    "# Let's not bother about it for now\n",
    "print check_match(plain_affinity, \" .056 M \")\n",
    "print check_match(plain_affinity, \".056 M \")\n",
    "\n",
    "print check_match(plain_affinity, \"6\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "s = open(\"../data/intact/pmid_10607564/sentences.txt\")\n",
    "\n",
    "def extract_affinities(context):\n",
    "    affinity = re.findall(\"[\\s\\b]K d[\\s\\b].*?[\\s\\b]nM\", context)\n",
    "    affinity = affinity + re.findall(\"[\\s\\b]K d[\\s\\b].*?[\\s\\b].M\", context)\n",
    "    return list(set(affinity))\n",
    "\n",
    "for line in s:\n",
    "    print line\n",
    "    print extract_affinities(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "for id in sample_ids:\n",
    "    print \"id:\", id\n",
    "    report_file = open(\"data/%s/report.txt\" % (id), \"w\")\n",
    "    intact_rows = open(\"data/%s/intact.txt\" % (id)).readlines()[1:]\n",
    "    sentence_indices = []\n",
    "    for intact_row in intact_rows:\n",
    "        cells = intact_row.split(\"\\t\")\n",
    "        participant_a, g_a, participant_b, g_b, affinity = cells[2], cells[1], cells[5], cells[4], cells[9]\n",
    "        plain_affinity = int(str(affinity).replace(\"0.\", \"\"))\n",
    "        print plain_affinity\n",
    "\n",
    "        # Sometimes the affinity value is 0. That would not identify any reasonable\n",
    "        # context, so we will ignore it for now\n",
    "        if plain_affinity != 0:    \n",
    "            sentences = open(\"data/%s/sentences.txt\" % (id)).readlines()\n",
    "            for sentence in sentences:\n",
    "                sentence_id, no_id_sentence = sentence.split(\" \", 1)\n",
    "\n",
    "                matches = check_match(plain_affinity, no_id_sentence)\n",
    "                if matches != [] and no_id_sentence.find(\"<sub>\") != -1:\n",
    "                    sentence_index = sentence_id.split(\".\")[-1]\n",
    "                    sentence_indices.append(int(sentence_index))\n",
    "                    print \"Found an affinity value matching the pattern... %s\" % (matches)\n",
    "                    print >>report_file, \"%s | %s | %s | %s | %s | %s | %s\\n\" % (plain_affinity, participant_a, g_a, participant_b, g_b, sentence_id, no_id_sentence.strip())\n",
    "\n",
    "    sorted_sentence_indices = sorted(set(sentence_indices))\n",
    "    print sorted_sentence_indices\n",
    "\n",
    "    from plumbum.cmd import ls\n",
    "    index_cards_cmd = ls[\"-1\", \"data/%s/index_cards/\" % (id)]\n",
    "    index_card_names = index_cards_cmd().split(\"\\n\")\n",
    "    #print index_card_names\n",
    "\n",
    "    index_card_sentence_indices = []\n",
    "    s_info = {}\n",
    "    import json\n",
    "    from pprint import pprint\n",
    "    for index_card_name in index_card_names:\n",
    "        if index_card_name.strip() != \"\":\n",
    "            with open(\"data/%s/index_cards/%s\" % (id, index_card_name)) as index_card_file:\n",
    "                index_card = json.load(index_card_file)\n",
    "                s_ids = index_card[\"sent_ids\"]\n",
    "                s_a = index_card[\"extracted_information\"][\"participant_a\"][\"entity_text\"]\n",
    "                s_b = index_card[\"extracted_information\"][\"participant_b\"][\"entity_text\"]\n",
    "                s_g_a = index_card[\"extracted_information\"][\"participant_a\"][\"identifier\"]\n",
    "                s_g_b = index_card[\"extracted_information\"][\"participant_b\"][\"identifier\"]\n",
    "                s_info[int(s_ids[0].split(\".\")[-1])] = (s_a, s_b, s_g_a, s_g_b)\n",
    "\n",
    "                for s_id in s_ids:\n",
    "                    s_index = int(s_id.split(\".\")[-1])\n",
    "                    index_card_sentence_indices.append(s_index)\n",
    "    sorted_index_card_sentence_indices = sorted(set(index_card_sentence_indices))\n",
    "    print sorted_index_card_sentence_indices\n",
    "\n",
    "    for g_index in sorted_sentence_indices:\n",
    "        for i_index in sorted_index_card_sentence_indices:\n",
    "            if abs(g_index - i_index) < 3:\n",
    "                print >>report_file, g_index, i_index, s_info[i_index]\n",
    "                zero_g_index = g_index - 1\n",
    "                zero_i_index = i_index - 1\n",
    "                print >>report_file, \"\".join(sentences[zero_i_index - 2:zero_i_index + 3])\n",
    "                print >>report_file\n",
    "    print \".\"*60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<!--\n",
    "### pmid_ ()\n",
    "#### Information in INTACT for this document (via intact.txt)\n",
    "\n",
    "<pre>\n",
    "</pre>\n",
    "\n",
    "\n",
    "#### Pattern match over document (.nxml)\n",
    "\n",
    "<pre>\n",
    "</pre>\n",
    "\n",
    "#### Potential sentence context (from sentences.txt) within reach +/-3 sentences of evidence sentence(s) in index_cards (.json)\n",
    "<pre>\n",
    "</pre>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Data Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### High Priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- **Spurious newlines**—There seem to be a lot of newline characters (`\\n`) in the evidence sentences. These do not appear in the original sentences.txt file. Find out whether these were introduced during AMR parsing or during interaction classification\n",
    "- **Encoding**—There are unicode character references (most likely greek characters, such as delta) in the evidence sentences inside the index card `.json` files. We should ensure that the encoding matches across the AMR and other training data.\n",
    "- **HTML Entities in the PubMed Articles**—Only a very small fraction of the 148 unique HTML entities in the biomedical articles have a reasonable mapping in ASCII land—using the ``unicodedata`` module in Python. <mark>It would be good to normalize them using a mapping table</mark>."
   ]
  },
  {
=======
>>>>>>> updated various data files in one swoop
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Error Analysis and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Sahil's Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Here is my understanding (I clarified it with Sahil, but, Sahil, feel free to correct me if I understood it wrongly)\n",
    "\n",
    "1. Ten documents **with at least 5/10 index cards per document** were randomly selected from the 82 documents which we were able to find in the public domain **and** in the .nxml format—out of the 500-odd unique documents representing the ~3K rows in the INTACT database (or, rather the spreadsheet). More specifics are present in the documents in the repository, so I am going with some approximations here.\n",
    "2. He manually counted the (micro) Precision across the document and it came to 57.35\n",
    "3. He did **not** check for the presence of binding affinity information or the context for K_d values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### My Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Note: All index cards only represent a top-level the \"bind\" relation. This seems somewhat contradictory to Sahil's information. *Need to clarify*\n",
    "\n",
    "1. I went through the same data again, but this time with four sentence of context before and after the sentence that contained the bind event.\n",
    "2. There were a total of 1492 (732 unique) lines of context\n",
    "3. Out of these only 53 (23 unique) lines had some variation of K_d value in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
    "( A ) SPR experiments for the interaction of WT NetrinG1 Lam–EGF1/4 and NetrinG2 Lam–EGF1 with WT NGL1/2 LRR–Ig reveals intra-class binding K d s in the low nanomolar range and cross-class binding K d s in the low micromolar range .\n",
    "( B ) The affinities depicted were normalized using the <strong style=\"background-color: red; color: white;\">K d</strong> value for WT intra-class binding .\n",
    "( a ) Surface plasmon resonance demonstrates that J-1 DSL-EGF3 , binds to CD46 1-4 coupled on the surface of the chip with a K D of ~8μM ( normalised by subtraction of mock-coupled channel ) .\n",
    "As a result of differences in the refractive index between the running buffer and the injected sample , the bulk shift in the SPR response was evaluated , and the k a and k d kinetic parameters were determined using the Biacore BIAevaluation 3.2 software .\n",
    "Consistent with previous reports ( Kim et al , 2006 ) , we find that the affinity between NGL3 and either NetrinG is very weak ( K d &gt ; 100 μM ) .\n",
    "Equilibrium dissociation constants ( K d ) for interactions among Myo1p tail , Wsp1p VCA , verprolin , Arp2/3 complex , and actin\n",
    "Free IL-15 bound to IL-2Rβ with a K D of 438 nM ( Fig . 4a left ) , consistent with prior SPR measurements for this interaction 25 .\n",
    "GST-Myo1p-23A bound Vrp1p with a higher affinity ( K d = 3–6 μM ) than muscle actin monomers or filaments ( K d &gt ; 20 μM ; Lee et al. , 2000 ) .\n",
    "However , the binding constants for IL-8 and its chemokine receptors are considerably higher ( K D = 1-7x10 -9 M ) [ 46 , 47 ] .\n",
    "In fact , p115 bound to GST–syntaxin-5 , with an apparent K d of 1.8 μM ( Fig . 6 C ) , and His–GOS-28 , with an apparent K d of 1.5 μM ( Fig . 6 D ) .\n",
    "Insert chart shows equilibrium values of binding and K D fit using SigmaPlot .\n",
    "Interestingly , IL-15—IL-15Rα complex bound to IL-2Rβ with a K D of 3 nM ( Fig . 4a right ) , an affinity increase of approximately 150-fold over free IL-15 .\n",
    "K d and maximum analyte binding ( B max ) values were obtained by non-linear curve fitting of a 1:1 Langmuir interaction model ( bound= B max / ( K d + C ) , where C is the analyte concentration calculated as monomer ) .\n",
    "Like human GST-WASp-VCA ( Hufner et al. , 2001 ) , GST-Wsp1p-VCA bound Arp2/3 complex much stronger ( K d ∼0.05 μM ) than GST-Myo1p-23A ( K d = 1–2 μM ) .\n",
    "PNK–FHA binds with modest affinity ( K d ∼10 μM ) to the XRCC1 FHA-binding region when phosphorylated on either Thr518 ( a ) or Thr523 ( b ) .\n",
    "PNK–FHA bound this with a comparable affinity ( K d = 1.70 µM ) to the equivalent wild-type peptide ( K d = 1.23 µM ) , but with a stoichiometry ( N = 0.91 ) indicative of a single site of binding , consistent with abrogation of the secondary binding site by the Thr523Val mutation ( Figure 2 E ) .\n",
    "The SPR analysis revealed that the binding of human IL-8 to Mtb AtsG arylsulfatase is characterized by a high binding affinity ( K D = 6.83x10 -6 M ) similar to that noted for the interaction of IL-8 with sulfated glycosaminoglycans , namely chondroitin-6-sulfate ( K D = 1.4±0.4x10 -6 M ) [ 44 ] and heparin ( K D = 2.0±0.4x10 -6 M ) [ 45 ] .\n",
    "The affinity of verprolin for actin monomers ( K d ∼1 μM ) was similar to other proteins with WH2 domains ( Hufner et al. , 2001 ; Marchand et al. , 2001 ; Martinez-Quiles et al. , 2001 ; Hertzog et al. , 2002 ) .\n",
    "The dissociation constant ( K D ) for the CD46-J-1 DSL-EGF3 interaction is about 8μM and with this in a less tight affinity range as the interaction between CD46 and C3b ( 1 μM , Dr. Claire Harris , University of Cardiff , UK ; personal communication ) but tighter than the assumed K D for interactions between sN-1 11-13 and J-1 DSL-EGF3 , where protein concentrations in excess of 50μM have previously been required to observe an interaction and the interaction was found not to be saturated with protein concentrations up to 160μM 13 .\n",
    "The estimated binding affinity of SahH to IL-8 is higher ( K D = 7.14x10 -10 M ) than the binding affinity of the chemokine with its specific receptors .\n",
    "The researchers determined that EGF binds specifically to mycobacteria with a K D of 2.0x10 -10 M and an average of 450±60 receptors per cell of M .\n",
    "Using purified , soluble , proteins to characterize interactions has demonstrated that the majority of cell-surface proteins interact with one another with K D s in the μM-range and concomitant fast on and off rates .\n",
    "When both sites are fully phosphorylated ( c ) , which is the likely result of the hierarchical phosphorylation of this region by CK2 in vivo , two FHA domains can bind with &gt ; 50-fold higher affinity ( K d ∼0.1 μM ) , suggesting the presence of a far more stable complex than had been suggested by previous studies .\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- We are identifying many false positive binding events. Because of that we end up generating a lot of pairs with the same K_d value for multiple pairs. This can be fixed by choosing only one pair per K_d value. I did not come across a case so far where multiple pairs have the same K_d value.\n",
    "- There are certainly binding events that are in the paper, but are absent from the INTACT database. That gives us a few correct answers.\n",
    "- We are also not recognizing negations and hedging that directly affect the binding event. This also generates false positives. There are some simple patterns such as \"do not bind\" or \"could bind\" that could filter out the obvious cases. Ideally the AMR should pick up on those negations.\n",
    "- There should be some easy cases that mistake the \"K d\" as some K-RAS-D protein. We should be able to get rid of these false positives through some data cleanup\n",
    "- A few cases are owing to encoding issues where the character \\u2013 which represents a em-dash got converted to the text u2013 and is part of an extraction. We should be able to fix this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Sample Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Here we will take a look at a few sample cases to get a sense of whether the information required to extract affinity is present together between the index card, document context and using the normalized values of entities. \n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### pmid_22242148 (Extractable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Information in INTACT for this document (via intact.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
<<<<<<< HEAD
    "direct interaction\tuniprotkb:O60687\t<strong style=\"color: red;\">SRPX2</strong>\tHomo sapiens (taxid:9606)\tuniprotkb:P14210\t<strong style=\"color: red;\">HGF</strong>\tHomo sapiens (taxid:9606)\t-\tkd\t<strong style=\"color: red;\">0.0000000056</strong>\t-\tbiosensor (MI:0968)\tpubmed:22242148\n",
    "</pre>"
=======
    "uniprotkb:O60687\t<strong style=\"color: red;\">SRPX2</strong>\tuniprotkb:P14210\t<strong style=\"color: red;\">HGF</strong>\t-\tkd\t<strong style=\"color: red;\">0.0000000056</strong>\t-\tpubmed:22242148\n",
    "</pre>\n",
    "</div>\n",
    "</div>"
>>>>>>> updated various data files in one swoop
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Pattern match over document (.nxml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
<<<<<<< HEAD
    "56 | SRPX2 | uniprotkb:O60687 | HGF | uniprotkb:<strong style=\"color: red;\">P14210</strong> | pmid_22242148.<strong style=\"color: red;\">106</strong> | The <i>K<sub>d</sub></i> value of this interaction, calculated from the ratio of <i>K<sub>diss</sub>/K<sub>ass</sub></i>, was <strong style=\"color: red;\">5.6</strong> nM; these data were similar to those for previously reported data on HGF and endocan <xref ref-type=\"bibr\" rid=\"pone.0027922-Bchard1\">[9]</xref>.\n",
    "</pre>"
=======
    "56 | SRPX2 | uniprotkb:O60687 | HGF | uniprotkb:<strong style=\"color: red;\">P14210</strong> | pmid_22242148.<strong style=\"color: red;\">106</strong> | \n",
    "The <i>K<sub>d</sub></i> value of this interaction, calculated from the ratio of <i>K<sub>diss</sub>/K<sub>ass</sub></i>, was <strong style=\"color: red;\">5.6</strong> nM; ...</xref>.\n",
    "</pre>\n",
    "</div>\n",
    "</div>"
>>>>>>> updated various data files in one swoop
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Potential sentence context (from sentences.txt) within reach +/-3 sentences of evidence sentence(s) in index_cards (.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, although the first index card hit includes the binding context, it does not seem to have evidence conntecing it to the affinity value. The second example seems to connect the two together. Although it talks about _GAGs of SRPX2_. No idea what that means..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
    "106 <strong style=\"color: red;\">104</strong> (u'HGF', u'CSPG-ENDOCAN', u'UNIPROT::HGF_HUMAN', u'UNIPROT::PGCA_HUMAN')\n",
    "\n",
    "pmid_22242148.102 <sec-title level=\"2\"><strong style=\"color: red;\">HGF binds to SRPX2</strong></sec-title>\n",
    "pmid_22242148.103 It is well known that several ligands including HGF, heparin-binding EGF-like growth factor, fibroblast growth factor 2 and vascular endothelial growth factor are capable of binding to the GAG chain and that such interactions are considered to be a unique characteristic of GAGs and proteoglycans <xref ref-type=\"bibr\" rid=\"pone.0027922-Yamada1\">[8]</xref>.\n",
    "pmid_22242148.<strong style=\"color: red;\">104</strong> According to a report on CSPG endocan and HGF binding <xref ref-type=\"bibr\" rid=\"pone.0027922-Bchard1\">[9]</xref>, we examined the interaction between HGF and GAGs using an IAsys resonant mirror biosensor.\n",
    "pmid_22242148.105 HGF dose-dependently bound to the GAGs of SRPX2, while control BSA did not (<xref ref-type=\"fig\" rid=\"pone-0027922-g005\">Fig. 5A</xref>).\n",
<<<<<<< HEAD
    "pmid_22242148.106 The <i>K<sub>d</sub></i> value of this interaction, calculated from the ratio of <i>K<sub>diss</sub>/K<sub>ass</sub></i>, was <strong style=\"color: red;\">5.6</strong> nM; these data were similar to those for previously reported data on HGF and endocan <xref ref-type=\"bibr\" rid=\"pone.0027922-Bchard1\">[9]</xref>.\n",
    "\n",
=======
    "pmid_22242148.106 The <i>K<sub>d</sub></i> value of this interaction, calculated from the ratio of <i>K<sub>diss</sub>/K<sub>ass</sub></i>, was <strong style=\"color: red;\">5.6</strong> nM; </xref>.\n",
    "<hr/>\n",
>>>>>>> updated various data files in one swoop
    "\n",
    "106 <strong style=\"color: red;\">105</strong> (u'<strong style=\"color: red;\">SRPX2</strong>', u'<strong style=\"color: red;\">HGF</strong>', u'UNIPROT::SRPX2_HUMAN', u'UNIPROT::HGF_HUMAN')\n",
    "\n",
    "pmid_22242148.103 It is well known that several ligands including HGF, heparin-binding EGF-like growth factor, fibroblast growth factor 2 and vascular endothelial growth factor are capable of binding to the GAG chain and that such interactions are considered to be a unique characteristic of GAGs and proteoglycans <xref ref-type=\"bibr\" rid=\"pone.0027922-Yamada1\">[8]</xref>.\n",
    "pmid_22242148.104 According to a report on CSPG endocan and HGF binding <xref ref-type=\"bibr\" rid=\"pone.0027922-Bchard1\">[9]</xref>, we examined the interaction between HGF and GAGs using an IAsys resonant mirror biosensor.\n",
    "pmid_22242148.<strong style=\"color: red;\">105</strong> <strong style=\"color: red;\">HGF dose-dependently bound to the GAGs of SRPX2</strong>, while control BSA did not (<xref ref-type=\"fig\" rid=\"pone-0027922-g005\">Fig. 5A</xref>).\n",
    "pmid_22242148.106 The <i>K<sub>d</sub></i> value of this interaction, calculated from the ratio of <i>K<sub>diss</sub>/K<sub>ass</sub></i>, was <strong style=\"color: red;\">5.6</strong> nM; these data were similar to those for previously reported data on HGF and endocan <xref ref-type=\"bibr\" rid=\"pone.0027922-Bchard1\">[9]</xref>.\n",
    "pmid_22242148.107 Next, we examined the biological function of SRPX2 on HGF.\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### pmid_21277013 (Not Extractable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "This example, we just have one index card and the grounded entities do not seem to be at all related to the pairs in the INTACT rows for the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Information in INTACT for this document (via intact.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
<<<<<<< HEAD
    "direct interaction\tuniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\tuniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\t-\tkd\t0.00011\t-\tisothermal titration calorimetry (MI:0065)\tpubmed:21277013\n",
    "direct interaction\tuniprotkb:A9CQL4\tCrSAS-6\tChlamydomonas reinhardtii (taxid:3055)\tuniprotkb:A9CQL4\tCrSAS-6\tChlamydomonas reinhardtii (taxid:3055)\t-\tkd\t0.000000000000000\t-\tisothermal titration calorimetry (MI:0065)\tpubmed:21277013\n",
    "direct interaction\tuniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\tuniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\t-\tkd\t0.0000009\t-\tcircular dichroism (MI:0016)\tpubmed:21277013\n",
    "direct interaction\tuniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\tuniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\t-\tkd\t0.00011\t-\tcosedimentation through density gradient (MI:0029)\tpubmed:21277013\n",
    "</pre>"
=======
    "uniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\t\tuniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\t\t-\tkd\t0.00011\t\t\t\t-\tpubmed:21277013\n",
    "uniprotkb:A9CQL4\tCrSAS-6\tChlamydomonas reinhardtii (taxid:3055)\tuniprotkb:A9CQL4\tCrSAS-6\tChlamydomonas reinhardtii (taxid:3055)\t-\tkd\t0.000000000000000\t-\tpubmed:21277013\n",
    "uniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\t\tuniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\t\t-\tkd\t0.0000009\t\t\t-\tpubmed:21277013\n",
    "uniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\t\tuniprotkb:O62479\tsas-6\tCaenorhabditis elegans (taxid:6239)\t\t-\tkd\t0.00011\t\t\t\t-\tpubmed:21277013\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n"
>>>>>>> updated various data files in one swoop
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Pattern match over document (.nxml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
    "<strong style=\"color: red;\">Did not find any matches</strong>\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Potential sentence context (from sentences.txt) within reach +/-3 sentences of evidence sentence(s) in index_cards (.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found some numerical matches for affinity, but no context sentences near the one identified in the index card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
    "11 | sas-6 | uniprotkb:O62479 | sas-6 | uniprotkb:O62479 | pmid_21277013.236 | Extended Experimental Procedures Cloning and Protein Preparation C. elegans &lt;i&gt;SAS-6 Proteins&lt;/i&gt;&lt;/title&gt;&lt;p&gt;DNA encoding full-length or fragments of &lt;i&gt;C. elegans&lt;/i&gt; SAS-6 (Uniprot ID O62479) were cloned in pET system vectors (Novagen) encoding for N-terminal His&lt;sub&gt;6&lt;/sub&gt;-tags or pGEX system vectors (GE healthcare) encoding for N-terminal GST-tags.\n",
    "\n",
    "9 | sas-6 | uniprotkb:O62479 | sas-6 | uniprotkb:O62479 | pmid_21277013.46 | Fitting of the data revealed a dissociation constant, K&lt;sub&gt;d&lt;/sub&gt;, of &lt;strong style=\"color: red;\"&gt;0.9 ± 0.1 μM&lt;/strong&gt; (&lt;xref ref-type=\"fig\" rid=\"fig1\"&gt;Figure 1&lt;/xref&gt;F).\n",
    "\n",
    "9 | sas-6 | uniprotkb:O62479 | sas-6 | uniprotkb:O62479 | pmid_21277013.178 | The structures of both proteins were solved by molecular replacement and refined to final &lt;i&gt;R&lt;/i&gt;&lt;sub&gt;work&lt;/sub&gt;/&lt;i&gt;R&lt;/i&gt;&lt;sub&gt;free&lt;/sub&gt; values of 18.1%/21.8% (crN) and 19.6%/22&lt;strong style=\"color: red;\"&gt;.9%&lt;/strong&gt; (crN-6HR[F145E]).\n",
    "\n",
    "&lt;strong style=\"background-color: red; color: white;\"&gt;In the following case, the numerical match was inside a xml element! Should ignore them while testing for matches&lt;/strong&gt;\n",
    "\n",
    "11 | sas-6 | uniprotkb:O62479 | sas-6 | uniprotkb:O62479 | pmid_21277013.236 | &lt;boxed-text id=\"dtbox1\"&gt;&lt;sec id=\"dtbox1sec1\"&gt;&lt;title&gt;Extended Experimental Procedures&lt;/title&gt;&lt;sec id=\"dtbox1sec1.1\"&gt;&lt;title&gt;Cloning and Protein Preparation&lt;/title&gt;&lt;sec id=\"dtbox1sec1.1.1\"&gt;&lt;title&gt;C. elegans &lt;i&gt;SAS-6 Proteins&lt;/i&gt;&lt;/title&gt;&lt;p&gt;DNA encoding full-length or fragments of &lt;i&gt;C. elegans&lt;/i&gt; SAS-6 (Uniprot ID O62479) were cloned in pET system vectors (Novagen) encoding for N-terminal His&lt;sub&gt;6&lt;/sub&gt;-tags or pGEX system vectors (GE healthcare) encoding for N-terminal GST-tags.\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### pmid_2118142 (Not Extractable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Information in INTACT for this document (via intact.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: red;\">There were no index cards produced</strong>. The text was just the abstract and not full text.\n",
    "<strong style=\"background-color: red; color: white\">Need to include a check that deals with documents with only abstract in a certain way</strong>---Especially if it does not generate any index cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
    "<pre>\n",
    "physical association\tuniprotkb:<strong style=\"color: red;\">P49024</strong>\t<strong style=\"color: red;\">PXN</strong>\tGallus gallus (Chicken) (taxid:9031)\tuniprotkb:<strong style=\"color: red;\">P12003</strong>\t<strong style=\"color: red;\">VCL</strong>\tGallus gallus (Chicken) (taxid:9031)\t-\tkd\t<strong style=\"color: red;\">0.00000006</strong>\t-\tsolid phase assay (MI:0892)\tpubmed:2118142\n",
    "</pre>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Pattern match over document (.nxml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
    "<strong style=\"color: red;\">Abstract does not contain the required information</strong>\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Potential sentence context (from sentences.txt) within reach +/-3 sentences of evidence sentence(s) in index_cards (.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
    "<strong style=\"color: red;\">Empty because no index card generated</strong>\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### pmid_21946559 (Not Extractable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could not find any information in the text. It appears that some binding affinity information is present in the figures of this paper, so at least for now, it is beyond the scope of our pursuit."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
<<<<<<< HEAD
    "I could not find any information in the text. It appears that some binding affinity information is present in the figures of this paper, so at least for now, it is beyond the scope of our pursuit.\n",
    "\n",
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/pmid_21946559.png\">\n",
    "\n",
    ".. image:: ../figures/pmid_21946559.png"
=======
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/pmid_21946559.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/pmid_21946559.png\">"
>>>>>>> updated various data files in one swoop
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Information in INTACT for this document (via intact.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
<<<<<<< HEAD
    "direct interaction\tuniprotkb:Q9HBW1\tLRRC4\tHomo sapiens (taxid:9606)\tuniprotkb:Q96CW9\tNTNG2\tHomo sapiens (taxid:9606)\t-\tkd\t<strong style=\"color: red;\">0.0000000073</strong>\t-\tsurface plasmon resonance (MI:0107)\tpubmed:21946559\n",
    "direct interaction\tuniprotkb:Q96CW9\tNTNG2\tHomo sapiens (taxid:9606)\tuniprotkb:Q9HCJ2\tLRRC4C\tHomo sapiens (taxid:9606)\t-\tkd\t<strong style=\"color: red;\">0.00000198</strong>\t-\tsurface plasmon resonance (MI:0107)\tpubmed:21946559\n",
    "direct interaction\tuniprotkb:Q9HBW1\tLRRC4\tHomo sapiens (taxid:9606)\tuniprotkb:Q96CW9\tNTNG2\tHomo sapiens (taxid:9606)\t-\tkd\t<strong style=\"color: red;\">0.0000000133</strong>\t-\tsurface plasmon resonance (MI:0107)\tpubmed:21946559\n",
    "direct interaction\tuniprotkb:Q9HBW1\tLRRC4\tHomo sapiens (taxid:9606)\tuniprotkb:Q9Y2I2\tNTNG1\tHomo sapiens (taxid:9606)\t-\tkd\t<strong style=\"color: red;\">0.0000049</strong>\t-\tsurface plasmon resonance (MI:0107)\tpubmed:21946559\n",
    "direct interaction\tuniprotkb:Q96CW9\tNTNG2\tHomo sapiens (taxid:9606)\tuniprotkb:Q9HCJ2\tLRRC4C\tHomo sapiens (taxid:9606)\t-\tkd\t<strong style=\"color: red;\">0.0000026</strong>\t-\tsurface plasmon resonance (MI:0107)\tpubmed:21946559\n",
    "direct interaction\tuniprotkb:Q9Y2I2\tNTNG1\tHomo sapiens (taxid:9606)\tuniprotkb:Q9HCJ2\tLRRC4C\tHomo sapiens (taxid:9606)\t-\tkd\t<strong style=\"color: red;\">0.0000000079</strong>\t-\tsurface plasmon resonance (MI:0107)\tpubmed:21946559\n",
    "direct interaction\tuniprotkb:Q9Y2I2\tNTNG1\tHomo sapiens (taxid:9606)\tuniprotkb:Q9HBW1\tLRRC4\tHomo sapiens (taxid:9606)\t-\tkd\t<strong style=\"color: red;\">0.0000052</strong>\t-\tsurface plasmon resonance (MI:0107)\tpubmed:21946559\n",
    "direct interaction\tuniprotkb:Q9Y2I2\tNTNG1\tHomo sapiens (taxid:9606)\tuniprotkb:Q9HCJ2\tLRRC4C\tHomo sapiens (taxid:9606)\t-\tkd\t<strong style=\"color: red;\">0.0000000125</strong>\t-\tsurface plasmon resonance (MI:0107)\tpubmed:21946559\n",
    "</pre>"
=======
    "uniprotkb:Q9HBW1\tLRRC4\t\tuniprotkb:Q96CW9\tNTNG2\t\t-\tkd\t<strong style=\"color: red;\">0.0000000073</strong>\t-\tpubmed:21946559\n",
    "uniprotkb:Q96CW9\tNTNG2\t\tuniprotkb:Q9HCJ2\tLRRC4C\t-\tkd\t<strong style=\"color: red;\">0.00000198</strong>\t-\t\tpubmed:21946559\n",
    "uniprotkb:Q9HBW1\tLRRC4\t\tuniprotkb:Q96CW9\tNTNG2\t\t-\tkd\t<strong style=\"color: red;\">0.0000000133</strong>\t-\tpubmed:21946559\n",
    "uniprotkb:Q9HBW1\tLRRC4\t\tuniprotkb:Q9Y2I2\tNTNG1\t\t-\tkd\t<strong style=\"color: red;\">0.0000049</strong>\t-\t\tpubmed:21946559\n",
    "uniprotkb:Q96CW9\tNTNG2\t\tuniprotkb:Q9HCJ2\tLRRC4C\t-\tkd\t<strong style=\"color: red;\">0.0000026</strong>\t-\t\tpubmed:21946559\n",
    "uniprotkb:Q9Y2I2\tNTNG1\t\tuniprotkb:Q9HCJ2\tLRRC4C\t-\tkd\t<strong style=\"color: red;\">0.0000000079</strong>\t-\tpubmed:21946559\n",
    "uniprotkb:Q9Y2I2\tNTNG1\t\tuniprotkb:Q9HBW1\tLRRC4\t\t-\tkd\t<strong style=\"color: red;\">0.0000052</strong>\t-\t\tpubmed:21946559\n",
    "uniprotkb:Q9Y2I2\tNTNG1\t\tuniprotkb:Q9HCJ2\tLRRC4C\t-\tkd\t<strong style=\"color: red;\">0.0000000125</strong>\t-\tpubmed:21946559\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n"
>>>>>>> updated various data files in one swoop
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Pattern match over document (.nxml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a numeric pattern, but it is a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
    "26 | NTNG2 | uniprotkb:Q96CW9 | LRRC4C | uniprotkb:Q9HCJ2 | pmid_21946559.24 | We present crystal structures of a NetrinG1<sub>Lam–EGF1</sub>+NGL1<sub>LRR–Ig</sub> complex, NetrinG2<sub>Lam–EGF1</sub>, a NetrinG2<sub>Lam–EGF1</sub>+NGL2<sub>LRR–Ig</sub> complex and NGL3<sub>LRR–Ig</sub> determined at resolutions of 3.3, 2.2, <strong style=\"color: red;\">2.6</strong> and 3.1 Å, respectively (<xref ref-type=\"table\" rid=\"t1\">Table I</xref>).\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "<pre>\n",
    "26 | NTNG2 | uniprotkb:Q96CW9 | LRRC4C | uniprotkb:Q9HCJ2 | pmid_21946559.24 | We present crystal structures of a NetrinG1<sub>Lam–EGF1</sub>+NGL1<sub>LRR–Ig</sub> complex, NetrinG2<sub>Lam–EGF1</sub>, a NetrinG2<sub>Lam–EGF1</sub>+NGL2<sub>LRR–Ig</sub> complex and NGL3<sub>LRR–Ig</sub> determined at resolutions of 3.3, 2.2, <strong style=\"color: red;\">2.6</strong> and 3.1 Å, respectively (<xref ref-type=\"table\" rid=\"t1\">Table I</xref>).\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Potential sentence context (from sentences.txt) within reach +/-3 sentences of evidence sentence(s) in index_cards (.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<div class=\"output_area docutils container\">\n",
    "<div class=\"highlight\">\n",
    "</div>\n",
    "</div>\n",
    "<pre>\n",
    "<strong style=\"color: red;\">None</strong>\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Specific cases of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Summary of Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/example-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/example-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/example-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/example-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "raw_mimetype": "text/html",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/example-3.png\">\n",
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/example-3a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/example-3.png\">\n",
    "<img src=\"http://cemantix.org/documents/isi---big-mechanism/figures/example-3a.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../unique-html-entities.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ee4206b1f035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../unique-html-entities.html\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../normalized-unique-html-entities.html\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../unique-html-entities.html'"
     ]
    }
   ],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "h=HTMLParser()\n",
    "\n",
    "import unicodedata\n",
    "from unicodedata import normalize\n",
    "\n",
    "import codecs \n",
    "\n",
    "with open(\"../unique-html-entities.html\") as f:\n",
    "    with codecs.open(\"../normalized-unique-html-entities.html\", \"w\", \"utf-8\") as o:\n",
    "        for line in f:            \n",
    "            print line.strip(), h.unescape(line.strip()), normalize('NFKD', h.unescape(line.strip())).encode('ascii', 'backslashreplace')\n",
    "            o.write(\"%s %s %s\\n <br/>\"% (line.strip(), h.unescape(line.strip()), normalize('NFKD', h.unescape(line.strip())).encode('ascii', 'backslashreplace')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The following is the steps in the extraction process:\n",
    "\n",
    "- For each index card\n",
    "  - Search for affinity values nearby using a few high-precision regular expression in the 4 sentence before and after context of the sentence that has been identified as containing a binding event. \n",
    "  - Associate that K_d value with the interacting pair in the binding event\n",
    "   \n",
    "Note: One limitation (among likely others) of this process is that the same K_d value can be associated with multiple pairs of participants in binding events. \n",
    "\n",
    "Finding that is true in the curent data sample, but might not be true for all cases—Although the index_card JSON field \"sent_ids\" indicates that there can be more than one sentences, currently all interactions are within a single sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Algorithm to extract K_d values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "code_folding": [
     22
    ],
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents missing context: 0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "dash_re = re.compile(\"^----+$\", re.MULTILINE)\n",
    "empty_line_re = re.compile(\"\\n\\n\", re.MULTILINE)\n",
    "\n",
    "with codecs.open(\"../all-with-context\", \"r\", \"utf-8\") as f:\n",
    "    lines = dash_re.split(f.read())\n",
    "\n",
    "human2uniprot_id_map = {}\n",
    "with open (\"../metadata/uniprot-all-human-id-map.tsv\") as mapping_file:\n",
    "    for mapping in mapping_file:\n",
    "        uniprot_id, human_id = mapping.split()\n",
    "        human2uniprot_id_map[human_id.lower()] = uniprot_id.lower()\n",
    "\n",
    "#pprint(peekdict(human2uniprot_id_map))\n",
    "        \n",
    "all_keys = human2uniprot_id_map.keys()\n",
    "\n",
    "def extract_affinities(context):\n",
    "    affinity = re.findall(\"[\\s\\b]K d[\\s\\b].*?[\\s\\b]nM\", context)\n",
    "    affinity = affinity + re.findall(\"[\\s\\b]K d[\\s\\b].*?[\\s\\b].M\", context)\n",
    "    return list(set(affinity))\n",
    "\n",
    "def get_center_index(contexts):\n",
    "    if len(contexts) > 4:\n",
    "        if int(sentid) < 5:\n",
    "            center = len(contexts) - 5\n",
    "        else:\n",
    "            center = 4\n",
    "        return center\n",
    "    else:\n",
    "        print contexts\n",
    "        center = len(contexts) - 1\n",
    "        return center\n",
    "        #raise Exception(\"too small file\")\n",
    "    \n",
    "\n",
    "def replace_with_uniprot_id(human_id):\n",
    "\n",
    "    with codecs.open(\"map-to-uniprot-id.log\", \"a\", \"utf-8\") as log:\n",
    "        if human_id.replace(\"::\", \":\").lower() in human2uniprot_id_map:\n",
    "            human_id = human2uniprot_id_map[human_id.replace(\"::\", \":\").lower()].upper()\n",
    "        else:\n",
    "            print >>log, \"[%s] mapping not found...\" % (human_id)\n",
    "        return human_id\n",
    "\n",
    "num_missing_context = 0\n",
    "with open (\"output.csv\", \"w\") as o:\n",
    "    with open(\"output.log\", \"w\") as log:\n",
    "        for line in lines[0:-1]:\n",
    "            line = line.replace(\"(\", \"_LRB_\")\n",
    "            line = line.replace(\")\", \"_RRB_\")\n",
    "            bits = empty_line_re.split(line.strip())\n",
    "            id, rest = bits[0].split(\" \", 1)\n",
    "            pmid, sentid = id.split(\".\")\n",
    "            rest_bits = rest.split()\n",
    "            assert len(rest_bits) == 4, \"Wrong number of fields [%s]\" % (rest_bits)\n",
    "\n",
    "            #print rest_bits[2], \"->\",\n",
    "            rest_bits[2] = replace_with_uniprot_id(rest_bits[2])\n",
    "            #print rest_bits[2]\n",
    "\n",
    "            #print rest_bits[3], \"->\",\n",
    "            rest_bits[3] = replace_with_uniprot_id(rest_bits[3])\n",
    "            #print rest_bits[3]\n",
    "            \n",
    "            rest = \" \".join(rest_bits)\n",
    "\n",
    "            if len(bits) != 2:\n",
    "                print \"[%s] Did not find any context...\" % (id)\n",
    "                num_missing_context = num_missing_context + 1\n",
    "                continue\n",
    "\n",
    "            contexts = bits[1].split(\"\\n\")\n",
    "            center = get_center_index(contexts)\n",
    "\n",
    "            found_affinity = False\n",
    "\n",
    "            extracted_affinities = []\n",
    "            extracted_affinities = extracted_affinities + extract_affinities(contexts[center])\n",
    "            if extracted_affinities == []:\n",
    "                found_affinity = False\n",
    "            else:\n",
    "                found_affinity = True\n",
    "\n",
    "            if found_affinity is False:\n",
    "                for i in range(center+1, len(contexts)):\n",
    "                    extracted_affinities = extracted_affinities + extract_affinities(contexts[i])\n",
    "                    if extracted_affinities != []:\n",
    "                        found_affinity = True\n",
    "                        break;\n",
    "\n",
    "            if found_affinity is False:\n",
    "                for i in range(0, center):\n",
    "                    extracted_affinities = extracted_affinities + extract_affinities(contexts[i])\n",
    "                    if extracted_affinities != []:\n",
    "                        found_affinity = True\n",
    "                        break;\n",
    "\n",
    "            if extracted_affinities != []:\n",
    "                for i in range(0, len(extracted_affinities)):\n",
    "                    extracted_affinities[i] = re.sub(\",[^\\d]+,\", \"\", extracted_affinities[i])\n",
    "\n",
    "                affinity_string = \"|\".join([x.replace(\" \", \"@@\").replace(\",\", \"__COMMA__\") for x in extracted_affinities])\n",
    "                out = \"%s,%s,%s\" % (\",\".join(rest.split()), affinity_string, id)\n",
    "            else:\n",
    "                out = \"%s,%s,%s\" % (\",\".join(rest.split()), \"-\", id)\n",
    "\n",
    "            print >>log, \"%s__NEWLINE____NEWLINE__%s__NEWLINE__%s\" % (out.replace(\"@@\", \" \"), \"__NEWLINE__\".join(contexts), \"-\"*80)\n",
    "            print >>o, out.replace(\"@@\", \" \")\n",
    "#             print >>log, \"%s,__NEWLINE____NEWLINE__%s__NEWLINE__%s\" % (out, \"__NEWLINE__\".join(contexts), \"-\"*80)\n",
    "#             print >>o, out\n",
    "\n",
    "            \n",
    "print \"Total number of documents missing context:\", num_missing_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "from plumbum import local\n",
    "ls = local[\"ls\"]\n",
    "\n",
    "import json\n",
    "\n",
    "from l2k2r2.analysis import Interaction\n",
    "\n",
    "\n",
    "def sort_sentence_ids(one, two):\n",
    "    one_filename, one_sentence_index = one.split(\".\")\n",
    "    two_filename, two_sentence_index = two.split(\".\")\n",
    "\n",
    "    if one_filename == two_filename:\n",
    "        return cmp(int(one_sentence_index), int(two_sentence_index))\n",
    "    else:\n",
    "        return(cmp(one_filename, two_filename))\n",
    "\n",
    "\n",
    "index_card_hash = defaultdict(list)\n",
    "sentences_hash = defaultdict(list)\n",
    "\n",
    "with local.cwd(\"../data/intact/\"):\n",
    "    files = [x for x in ls().strip().split(\"\\n\") if x.startswith(\"pmid_\")]\n",
    "    for file in files:\n",
    "        with open(\"%s/sentences.txt\" % (file)) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                file_sentence_id, sentence = line.split(\" \", 1)\n",
    "                filename, sentence_id = file_sentence_id.split(\".\")\n",
    "\n",
    "                # Let's maintain the 1-indexed sentnce numbers\n",
    "                if i == 0:\n",
    "                    sentences_hash[filename].append(\"%d: None\" % (i))\n",
    "                assert int(\n",
    "                    i + 1) == int(sentence_id), \"The indices don't match [%s] [%s]\" % (i, sentence_id)\n",
    "                sentences_hash[filename].append(\n",
    "                    \"%s: %s\" % (sentence_id, sentence))\n",
    "        with local.cwd(\"%s/index_cards\" % (file)):\n",
    "            if ls().strip() != \"\":\n",
    "                index_card_filenames = ls().strip().split(\"\\n\")\n",
    "                index_card_filenames = [\n",
    "                    x for x in index_card_filenames if not x.endswith(\"~\")]\n",
    "                # print index_card_filenames\n",
    "                if index_card_filenames is not []:\n",
    "                    for index_card_filename in index_card_filenames:\n",
    "                        # print index_card_filename\n",
    "                        with open(\"%s\" % (index_card_filename)) as index_card_file:\n",
    "                            index_card_string = index_card_file.read()\n",
    "                            index_card = json.loads(index_card_string)\n",
    "# print\n",
    "# index_card[\"extracted_information\"][\"participant_a\"][\"entity_text\"]\n",
    "                            index_card_hash[filename].append(Interaction.from_json(index_card))\n",
    "#            else:\n",
    "#                print \"No index card found\"\n",
    "\n",
    "for pmid in index_card_hash.keys()[0:2]:\n",
    "    print pmid\n",
    "    for index_card in index_card_hash[pmid]:\n",
    "\n",
    "        filename, sentence_index = pmid, int(index_card.s_ids[0].split(\".\")[-1])\n",
    "        print sentence_index, len(sentences_hash[filename])\n",
    "        l = sentence_index - 4\n",
    "        if l < 1:\n",
    "            l = 1\n",
    "\n",
    "        h = sentence_index + 5\n",
    "        if h > len(sentences_hash[filename]):\n",
    "            h = len(sentences_hash[filename])\n",
    "\n",
    "        print index_card.s_ids[0], index_card.a, index_card.b, index_card.ga, index_card.gb\n",
    "        print sentence_index, l, h\n",
    "        for sentence in sentences_hash[filename][l:h]:\n",
    "            print sentence.strip()\n",
    "        print\n",
    "\n",
    "# for key in sorted(sentences_hash.keys()):\n",
    "#     print key, sentences_hash[key][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Normalize the extracted Kd values in the papers to molar (M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "import csvkit\n",
    "reader = csvkit.reader(open(\"output.csv\"))\n",
    "#reader.next() # Skip the header\n",
    "\n",
    "with open(\"predicted-affinity-values.csv\", \"w\") as p: \n",
    "    writer = csvkit.writer(p)\n",
    "    for row in reader:\n",
    "        if row[4].strip() != \"-\":\n",
    "            if row[4].find(\"nM\") != -1:\n",
    "                x = row[4].split()[0]\n",
    "                n = float(x) * 0.000000001\n",
    "                row[4] = \"%.10f\" % (n)\n",
    "            if row[4].find(\"mM\") != -1:\n",
    "                x = row[4].split()[0]\n",
    "                n = float(x) * 0.000001\n",
    "                row[4] = \"%.10f\" % (n)\n",
    "            if row[4].find(\"M\") != -1:\n",
    "                x = row[4].split()[0]\n",
    "                n = float(x) * 1\n",
    "                row[4] = \"%.10f\" % (n)\n",
    "\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Try to locate the Kd values mentioned in INTACT in the corresponding document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### First attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "We will try to get a sense of how good the recall is by looking at the gold INTACT binding and affinity values and checking whether we were able to extract a _binding_ event within a reasonable context (a +/-2 sentence window). This does not even consider whether the participants of the _binding_ event are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -1 data | shuf | head -n 14 | tee sample-ids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "sample_ids = [\"pmid_21277013\"]\n",
    "\n",
    "with open(\"sample-ids.txt\") as sample_ids_file:\n",
    "    sample_ids += [x.strip() for x in sample_ids_file.readlines()]\n",
    "print sample_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### A function to extract lines in the .nxml file containing annotated affinity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "plain_affinity = \"56\"\n",
    "def check_match(plain_affinity, sentence_string):\n",
    "    numbers = list(str(plain_affinity))\n",
    "    affinity_re = \"(?:\\.)?\".join(numbers)\n",
    "    #affinity_re = \"(?:^|\\n|\\s|-|\\+|\\.|;|:|,|\\(|\\\")(?:\\.)?%s(?:-|\\+|;|:|,|\\(|\\\"|\\.|\\s|\\n|$)\" % (affinity_re)\n",
    "    affinity_re = \"(?:^|\\W)(?:\\.)?%s(?:\\W|$)\" % (affinity_re)\n",
    "    #print affinity_re\n",
    "    matches = re.findall(affinity_re, sentence_string)\n",
    "    filtered_matches = [x for x in matches if not (x.startswith(\">\") or x.endswith(\"<\"))]\n",
    "    return filtered_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cases for various patterns in text\n",
    "# These are identified\n",
    "print check_match(plain_affinity, \" 56 nM \")\n",
    "print check_match(plain_affinity, \" (56 nM) \")\n",
    "print check_match(plain_affinity, \" 56.0 nM \")\n",
    "print check_match(plain_affinity, \" 5.6 nM \")\n",
    "print check_match(plain_affinity, \" .56 nM \")\n",
    "print check_match(plain_affinity, \"-56 nM \")\n",
    "print check_match(plain_affinity, \"56,\")\n",
    "print check_match(plain_affinity, \"+5.6 nM \")\n",
    "print check_match(plain_affinity, \"\\n.56 nM \")\n",
    "\n",
    "# The following are deliberately filtered out\n",
    "print check_match(plain_affinity, \">56<nM \")\n",
    "\n",
    "# The following cases are not captured (because of the addition of 0)\n",
    "# Let's not bother about it for now\n",
    "print check_match(plain_affinity, \" .056 M \")\n",
    "print check_match(plain_affinity, \".056 M \")\n",
    "\n",
    "print check_match(plain_affinity, \"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "s = open(\"../data/intact/pmid_10607564/sentences.txt\")\n",
    "\n",
    "def extract_affinities(context):\n",
    "    affinity = re.findall(\"[\\s\\b]K d[\\s\\b].*?[\\s\\b]nM\", context)\n",
    "    affinity = affinity + re.findall(\"[\\s\\b]K d[\\s\\b].*?[\\s\\b].M\", context)\n",
    "    return list(set(affinity))\n",
    "\n",
    "for line in s:\n",
    "    print line\n",
    "    print extract_affinities(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Second attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "import re\n",
    "import csvkit\n",
    "from glob import glob\n",
    "from plumbum import local\n",
    "import codecs\n",
    "from colorama import init\n",
    "from termcolor import colored\n",
    "init()\n",
    "\n",
    "ls = local[\"ls\"]\n",
    "pwd = local[\"pwd\"]\n",
    "\n",
    "from collections import defaultdict\n",
    "reader = csvkit.reader(open(\"../corpora/intact---korkut/gold-standard-intact-binding-affinities.csv\"))\n",
    "\n",
    "#skip header\n",
    "reader.next() \n",
    "\n",
    "def extract_affinities(context):\n",
    "    affinity = re.findall(\"[\\s\\b]K d[\\s\\b].*?[\\s\\b]nM\", context)\n",
    "    affinity = affinity + re.findall(\"[\\s\\b]K d[\\s\\b].*?[\\s\\b].M\", context)\n",
    "    return list(set(affinity))\n",
    "\n",
    "def check_match(plain_affinity, sentence_string):\n",
    "    numbers = list(str(plain_affinity))\n",
    "    affinity_re = \"(?:\\.)?\".join(numbers)\n",
    "    #affinity_re = \"(?:^|\\n|\\s|-|\\+|\\.|;|:|,|\\(|\\\")(?:\\.)?%s(?:-|\\+|;|:|,|\\(|\\\"|\\.|\\s|\\n|$)\" % (affinity_re)\n",
    "    affinity_re = \"(?:^|\\W)(?:\\.)?%s(?:\\W|$)\" % (affinity_re)\n",
    "    matches = re.finditer(affinity_re, sentence_string)\n",
    "    filtered_matches = [x for x in matches if not (x.group(0).startswith(\">\") or x.group(0).endswith(\"<\"))]\n",
    "    return filtered_matches\n",
    "\n",
    "def remove_decimals(value):\n",
    "    return re.sub(\"^0.0+\", \"\", value)\n",
    "\n",
    "\n",
    "ids_with_nxmls = []\n",
    "\n",
    "with local.cwd(\"../data/intact/\"):\n",
    "    files = ls[glob(\"*/sentences.txt\")]().split(\"\\n\")\n",
    "\n",
    "    for file in files:\n",
    "        ids_with_nxmls.append(file.split(\"/\")[0])\n",
    "    \n",
    "pmid2values = defaultdict(list)\n",
    "\n",
    "for row in reader:\n",
    "    pmid2values[row[12].replace(\"pubmed:\", \"pmid_\")].append([remove_decimals(row[9]), row])\n",
    "    \n",
    "    \n",
    "\n",
    "def id_and_sentence(sentence):\n",
    "    i, l = sentence.split(\" \", 1)\n",
    "    return i, l\n",
    "    \n",
    "docid_kd_hash = defaultdict(list)\n",
    "\n",
    "with codecs.open(\"../reports/searching-sentences-for-kd-values-in-intact-for-a-given-pmid.txt\", \"w\", \"utf-8\") as w:\n",
    "    #for id in pmid2values.keys()[:100]:\n",
    "    for id in pmid2values:\n",
    "        print >>w, \"-\"*100\n",
    "        print >>w, id\n",
    "\n",
    "        if len(pmid2values[id]) < 100 and id in ids_with_nxmls:\n",
    "        #if len(pmid2values[item]) == 1 and id in ids_with_nxmls:\n",
    "\n",
    "            for a_id, rest in pmid2values[id]:\n",
    "                b = [2,5,1,4,9]\n",
    "                selected_values = [rest[i] for i in b]\n",
    "                print >>w, \" \".join(selected_values)                \n",
    "                print selected_values\n",
    "                \n",
    "                found_match = False\n",
    "                with codecs.open(\"../data/intact/%s/sentences.txt\" % (id), \"r\", \"utf-8\") as file:\n",
    "                    print >>w, a_id\n",
    "                    print >>w\n",
    "\n",
    "                    lines = file.readlines()\n",
    "\n",
    "                    for i in range(0, len(lines)):\n",
    "                        sentence_id, sentence = id_and_sentence(lines[i])\n",
    "        \n",
    "                        ea = extract_affinities(sentence)\n",
    "                        cm = check_match(a_id, sentence)\n",
    "\n",
    "                        if cm != []:\n",
    "                            for cm_i in cm:\n",
    "                                \n",
    "                                chars = list(sentence)                                \n",
    "\n",
    "                                likely_kd = False\n",
    "                                for char in chars[int(cm_i.end(0)):int(cm_i.end(0)) + 10]:\n",
    "                                    if char == \"M\":\n",
    "                                        likely_kd = True\n",
    "                                                                        \n",
    "                                if likely_kd == True:                                \n",
    "                                        \n",
    "                                    #print >>w, chars\n",
    "                                    chars[cm_i.end(0):cm_i.end(0)] = colored(\"-))  \", \"red\")\n",
    "                                    chars[cm_i.start(0):cm_i.start(0)] = colored(\"  ((-\", \"red\")\n",
    "                                    \n",
    "                                    context = []\n",
    "                                    \n",
    "                                    if i - 2 > 0: \n",
    "                                        print >>w, lines[i-2].strip()\n",
    "                                        context.append(lines[i-2].strip())\n",
    "                                    if i - 1 > 0: \n",
    "                                        print >>w, lines[i-1].strip()                                        \n",
    "                                        context.append(lines[i-1].strip())\n",
    "\n",
    "                                    print >>w, sentence_id, \"\".join(chars).strip()\n",
    "                                    context.append(\"%s %s\" % (sentence_id, \"\".join(chars).strip()))\n",
    "\n",
    "                                    if i + 1 < len(lines):\n",
    "                                        print >>w, lines[i+1].strip()\n",
    "                                        context.append(lines[i+1].strip())                                                            \n",
    "                        \n",
    "                                    if i + 2 < len(lines): \n",
    "                                        print >>w, lines[i+2].strip()\n",
    "                                        context.append(lines[i+2].strip())\n",
    "                                    \n",
    "                                    a_key = \"%s-%s\" % (id, selected_values[-1])\n",
    "                                    docid_kd_hash[a_key].append(context)\n",
    "                                    print a_key\n",
    "                                    print \"\\n\".join(context)\n",
    "                                    print \"-\"*100\n",
    "\n",
    "                                    print >>w, \"\"\n",
    "                                    print >>w, \".\"*100\n",
    "                                    print >>w, \"\"\n",
    "                            \n",
    "                            found_match = True\n",
    "                        \n",
    "                        if ea != []:\n",
    "                            print >>w, \"|\".join(ea)\n",
    "                            print >>w, \"\"\n",
    "\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                if found_match is False:\n",
    "                    print >>w, \"\"\n",
    "                    print >>w, id, \"None\"\n",
    "                    print >>w, \"\"                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Try replacing INTACT grounded values with predicted ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "In addition to running the experiment to normalize the intact values for a better comparison, this also generates reports for the three cases where non, one and both participants get a match, with and without expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Number of string matches and ID matches 2\n",
      "Number of string matches only           4\n",
      "\n",
      "Number of gold and predicted pairs compared 77\n",
      "\n",
      "Number of unique document ID and Kd values in INTACT: 45\n",
      "Number of                        Kd values in INTACT 55\n",
      "\n",
      "Number of unique document ID and Kd values in PREDICTED INTACT: 44\n",
      "Number of                        Kd values in PREDICTED INTACT 70\n",
      "\n",
      "With Expansion:\n",
      "Number of times the key was     found in predicted 47\n",
      "Number of times the key was not found in predicted 8\n",
      "\n",
      "Number of times one of the two participant IDs matched 17\n",
      "Number of times           both participant IDs matched 7\n",
      "Number of times    none of the participant IDs matched 53\n",
      "\n",
      "Number predicted instances compared to gold instances 77\n",
      "(This should match the sum of earlier three possible cases)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import re\n",
    "import sys\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "\n",
    "def check_match(plain_affinity, sentence_string):\n",
    "    numbers = list(str(plain_affinity))\n",
    "    affinity_re = \"(?:\\.)?\".join(numbers)\n",
    "    #affinity_re = \"(?:^|\\n|\\s|-|\\+|\\.|;|:|,|\\(|\\\")(?:\\.)?%s(?:-|\\+|;|:|,|\\(|\\\"|\\.|\\s|\\n|$)\" % (affinity_re)\n",
    "    affinity_re = \"(?:^|\\W)(?:\\.)?%s(?:\\W|$)\" % (affinity_re)\n",
    "    #print affinity_re\n",
    "    matches = re.findall(affinity_re, sentence_string)\n",
    "    filtered_matches = [x for x in matches if not (x.startswith(\">\") or x.endswith(\"<\"))]\n",
    "    return filtered_matches\n",
    "\n",
    "def get_index_cards_for_pmid(id):\n",
    "    index_card_sentence_indices = []\n",
    "    s_info = {}\n",
    "    import json\n",
    "    from pprint import pprint\n",
    "    for index_card_name in index_card_names:\n",
    "        if index_card_name.strip() != \"\":\n",
    "            with open(\"../data/intact/%s/index_cards/%s\" % (id, index_card_name)) as index_card_file:\n",
    "                index_card = json.load(index_card_file)\n",
    "                s_ids = index_card[\"sent_ids\"]\n",
    "                s_a = index_card[\"extracted_information\"][\"participant_a\"][\"entity_text\"]\n",
    "                s_b = index_card[\"extracted_information\"][\"participant_b\"][\"entity_text\"]\n",
    "                s_g_a = index_card[\"extracted_information\"][\"participant_a\"][\"identifier\"]\n",
    "                s_g_b = index_card[\"extracted_information\"][\"participant_b\"][\"identifier\"]\n",
    "                s_info[int(s_ids[0].split(\".\")[-1])] = (s_a, s_b, s_g_a, s_g_b)\n",
    "\n",
    "                for s_id in s_ids:\n",
    "                    s_index = int(s_id.split(\".\")[-1])\n",
    "                    index_card_sentence_indices.append(s_index)\n",
    "    sorted_index_card_sentence_indices = sorted(set(index_card_sentence_indices))\n",
    "#     print sorted_index_card_sentence_indices\n",
    "    return s_info, sorted_index_card_sentence_indices\n",
    "\n",
    "\n",
    "# FIXME: THere is a weird dependence with the for id in loop where index_card_names get set\n",
    "sample_ids = [\"pmid_21277013\"]\n",
    "# sample_ids = []\n",
    "\n",
    "def get_plain_affinity(affinity):\n",
    "    return int(str(affinity).replace(\".\", \"\"))\n",
    "\n",
    "def get_sentences_and_matching_sentence_indices_for_pmid(id, affinity):\n",
    "    sentences = open(\"../data/intact/%s/sentences.txt\" % (id)).readlines()\n",
    "    plain_affinity = get_plain_affinity(affinity)\n",
    "    sentence_indices = []\n",
    "    for sentence in sentences:\n",
    "        sentence_id, no_id_sentence = sentence.split(\" \", 1)        \n",
    "        matches = check_match(plain_affinity, no_id_sentence)\n",
    "        if matches != [] and no_id_sentence.find(\"<sub>\") != -1:\n",
    "            sentence_index = sentence_id.split(\".\")[-1]\n",
    "            sentence_indices.append(int(sentence_index))\n",
    "#             print \"Found an affinity value matching the pattern... %s\" % (matches)\n",
    "#             print >>report_file, \"%s | %s | %s | %s | %s | %s | %s\\n\" % (plain_affinity, participant_a, g_a, participant_b, g_b, sentence_id, no_id_sentence.strip())\n",
    "    return sentences, sentence_indices\n",
    "\n",
    "\n",
    "for id in sample_ids:\n",
    "#     print \"id:\", id\n",
    "    report_file = open(\"../data/intact/%s/report.txt\" % (id), \"w\")\n",
    "    intact_rows = open(\"../data/intact/%s/intact.txt\" % (id)).readlines()[1:]\n",
    "    sentence_indices = []\n",
    "    for intact_row in intact_rows:\n",
    "        cells = intact_row.split(\"\\t\")\n",
    "        participant_a, g_a, participant_b, g_b, affinity = cells[2], cells[1], cells[5], cells[4], cells[9]\n",
    "        plain_affinity = get_plain_affinity(affinity)\n",
    "#         print plain_affinity\n",
    "\n",
    "        # Sometimes the affinity value is 0. That would not identify any reasonable\n",
    "        # context, so we will ignore it for now\n",
    "        if plain_affinity != 0:                \n",
    "            sentences, indices = get_sentences_and_matching_sentence_indices_for_pmid(id, plain_affinity)\n",
    "            sentence_indices = sentence_indices + indices\n",
    "                            \n",
    "    sorted_sentence_indices = sorted(set(sentence_indices))\n",
    "    print >>report_file, \"ssi:\", sorted_sentence_indices\n",
    "\n",
    "\n",
    "    from plumbum.cmd import ls\n",
    "    index_cards_cmd = ls[\"-1\", \"../data/intact/%s/index_cards/\" % (id)]\n",
    "    index_card_names = index_cards_cmd().split(\"\\n\")\n",
    "    #print index_card_names\n",
    "\n",
    "        \n",
    "    \n",
    "    s_info, sorted_index_card_sentence_indices = get_index_cards_for_pmid(id)\n",
    "#     print s_info\n",
    "#     for index in sorted_index_card_sentence_indices:\n",
    "#         print sentences[index]\n",
    "\n",
    "    for g_index in sorted_sentence_indices:\n",
    "        for i_index in sorted_index_card_sentence_indices:\n",
    "            if abs(g_index - i_index) < 3:\n",
    "                print >>report_file, g_index, i_index, s_info[i_index]\n",
    "                zero_g_index = g_index - 1\n",
    "                zero_i_index = i_index - 1\n",
    "                print >>report_file, \"\".join(sentences[zero_i_index - 2:zero_i_index + 3])\n",
    "                print >>report_file\n",
    "    print >>report_file, \".\"*60\n",
    "\n",
    "    \n",
    "    \n",
    "#---- moved cell\n",
    "DEBUG = False\n",
    "\n",
    "human2uniprot_id_map = {}\n",
    "\n",
    "#with open (\"../metadata/uniprot-all-human-id-map.tsv\") as mapping_file:\n",
    "with open (\"../metadata/uniprot-all-reviewed-id-map.tsv\") as mapping_file:\n",
    "    for mapping in mapping_file:\n",
    "        if mapping.strip() == \"\":\n",
    "            continue\n",
    "        uniprot_id, human_id = mapping.strip().split(\"\\t\")\n",
    "        human2uniprot_id_map[human_id.lower()] = uniprot_id.lower()\n",
    "\n",
    "all_keys = human2uniprot_id_map.keys()\n",
    "\n",
    "# for key in all_keys[0:10]:\n",
    "#     print key, human2uniprot_id_map[key]\n",
    "\n",
    "\n",
    "# read the n-best groundings for each unique protein name in the intact database\n",
    "protein2nbest_uniprot_id_map = defaultdict(list)\n",
    "\n",
    "#with open(\"../tasks/evaluate-extractions/all-ids---unique-intact-protein-names.txt\") as nbest_groundings:    \n",
    "with codecs.open(\"../tasks/evaluate-extractions/all-ids---unique-intact-protein-names.tsv\", \"r\", \"utf-8\") as nbest_groundings, codecs.open(\"../tasks/evaluate-extractions/ignored-groundings.log\", \"w\", \"utf-8\") as ignored, codecs.open(\"../tasks/evaluate-extractions/missing-groundings.log\", \"w\", \"utf-8\") as missing:    \n",
    "    for i, item in enumerate(nbest_groundings):\n",
    "        name, _, score, human_uniprot_id = item.strip().split(\"\\t\")        \n",
    "        #print name, _, score, human_uniprot_id\n",
    "        if float(score) > 60:\n",
    "            human_uniprot_id = \"uniprot:%s\" % (human_uniprot_id.lower())\n",
    "            if human_uniprot_id in human2uniprot_id_map:\n",
    "                uniprot_id = human2uniprot_id_map[human_uniprot_id]\n",
    "                #print name, _, score, human_uniprot_id, uniprot_id\n",
    "                #print\n",
    "            else:\n",
    "                print >>missing, \"id [%s] not found in the map\" % (human_uniprot_id)\n",
    "\n",
    "            protein2nbest_uniprot_id_map[name].append(uniprot_id)\n",
    "        else:\n",
    "            print >>ignored, \"not adding mapping for protein [%s] to map because of low score [%f]\" % (human_uniprot_id, float(score))\n",
    "            \n",
    "            \n",
    "#---- moved cell\n",
    "DEBUG = False\n",
    "\n",
    "# open the predicted and intact csv files\n",
    "import csvkit\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#---- moved cell\n",
    "DEBUG = False\n",
    "\n",
    "g_reader = csvkit.reader(codecs.open(\"../tasks/evaluate-extractions/intact-subset-matching-predicted-kd-values.csv\", \"r\", \"utf-8\"))\n",
    "p_reader = csvkit.reader(codecs.open(\"../tasks/evaluate-extractions/predicted-subset-matching-intact-kd-values.csv\", \"r\", \"utf-8\"))\n",
    "\n",
    "g_header = g_reader.next() \n",
    "p_header = p_reader.next() \n",
    "\n",
    "if DEBUG is True: print g_header\n",
    "if DEBUG is True: print p_header\n",
    "\n",
    "g_docid_kd_hash = defaultdict(list)\n",
    "\n",
    "for row in g_reader:\n",
    "    key = \"%s---%s\" % (row[-1], row[-2])\n",
    "    g_docid_kd_hash[key].append(row)\n",
    "\n",
    "if DEBUG is True:\n",
    "    for key in g_docid_kd_hash:\n",
    "        if len(g_docid_kd_hash[key]) > 0:\n",
    "            print key, g_docid_kd_hash[key]\n",
    "\n",
    "print \"-\"*100\n",
    "\n",
    "p_docid_kd_hash = defaultdict(list)\n",
    "\n",
    "#\n",
    "if DEBUG is True:\n",
    "    for key in protein2nbest_uniprot_id_map:\n",
    "        print key, protein2nbest_uniprot_id_map[key]\n",
    "\n",
    "#\n",
    "for row in p_reader:\n",
    "    key = \"%s---%s\" % (row[-1], row[-2])\n",
    "    p_docid_kd_hash[key].append(row)\n",
    "\n",
    "if DEBUG is True:\n",
    "    for key in p_docid_kd_hash:\n",
    "        if len(p_docid_kd_hash[key]) > 0:\n",
    "            print key, p_docid_kd_hash[key]\n",
    "\n",
    "            \n",
    "            \n",
    "def get_matches(p_name, p_id, expand=False):\n",
    "    p_id = p_id.lower()\n",
    "    \n",
    "    if expand is True:    \n",
    "        if p_name in protein2nbest_uniprot_id_map:                    \n",
    "            matches = protein2nbest_uniprot_id_map[p_name]\n",
    "            if not p_id in matches:\n",
    "                matches.append(p_id)\n",
    "            return matches\n",
    "    return [p_id]\n",
    "    \n",
    "# go through the list of intact document-kd \n",
    "i = 0\n",
    "\n",
    "a = 0\n",
    "ab = 0\n",
    "nab = 0\n",
    "\n",
    "gn = len(g_docid_kd_hash.keys())\n",
    "\n",
    "gnn = 0\n",
    "for key in g_docid_kd_hash:\n",
    "    gnn = gnn + len(g_docid_kd_hash[key])\n",
    "\n",
    "pn = len(p_docid_kd_hash.keys())\n",
    "pnn = 0\n",
    "\n",
    "for key in p_docid_kd_hash:\n",
    "    pnn = pnn + len(p_docid_kd_hash[key])\n",
    "\n",
    "    \n",
    "ssn = 0\n",
    "x = 0\n",
    "t = 0\n",
    "np = 0 # number of times gold key not found in predicted\n",
    "pp = 0 # number of times gold key found in predicted\n",
    "\n",
    "EXPAND = True\n",
    "\n",
    "if EXPAND is True:\n",
    "    oa_filename = \"one-match.with-expansion.txt\"\n",
    "    oab_filename = \"both-match.with-expansion.txt\"\n",
    "    onab_filename = \"none-match.with-expansion.txt\"\n",
    "else:\n",
    "    oa_filename = \"one-match.without-expansion.txt\"\n",
    "    oab_filename = \"both-match.without-expansion.txt\"\n",
    "    onab_filename = \"none-match.without-expansion.txt\"\n",
    "\n",
    "    \n",
    "def record_instances(g, p, expa, expb, o):\n",
    "    o.write(\"    g: %s\\n\" % (g))\n",
    "    o.write(\"    p: %s\\n\\n\" % (p))\n",
    "    o.write(\"ex-pa: %s -> %s\\n\" % (p[0], expa))\n",
    "    o.write(\"ex-pb: %s -> %s\\n\\n\" % (p[1], expb))\n",
    "\n",
    "    docid = g[-1]\n",
    "    \n",
    "    o.write(\"   kd: %s\\n\" % (docid))\n",
    "\n",
    "    s, m = get_sentences_and_matching_sentence_indices_for_pmid(docid, g[-2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    for m_i in m:\n",
    "        o.write(\"%d %d: %s\\n\" % (get_plain_affinity(g[-2]), m_i, s[m_i-1].strip()))\n",
    "    \n",
    "#     o.write(str(\"\\n\".join(s)))\n",
    "#     o.write(\"\\n\")\n",
    "#     o.write(\"%s\\n\" % (str(len(s))))\n",
    "#     o.write(\"%s\\n\" % (str(m)))\n",
    "        \n",
    "    o.write(\"-\"*100)\n",
    "    o.write(\"\\n\")\n",
    "\n",
    "    \n",
    "    \n",
    "match_hash = defaultdict(list)\n",
    "    \n",
    "with codecs.open(oa_filename, \"w\", \"utf-8\") as oa, codecs.open(oab_filename, \"w\", \"utf-8\") as oab, codecs.open(onab_filename, \"w\", \"utf-8\") as onab:\n",
    "    for key in g_docid_kd_hash:\n",
    "        for g_instance in g_docid_kd_hash[key]:\n",
    "            g_key = \"--\".join(g_instance)\n",
    "\n",
    "            found_some_match_flag = False\n",
    "\n",
    "            if DEBUG is True: print \"g:\", key, g_instance\n",
    "            if key in p_docid_kd_hash:\n",
    "                pp = pp + 1\n",
    "                for p_instance in p_docid_kd_hash[key]:\n",
    "                    \n",
    "                    \n",
    "                    p_key = \"--\".join(p_instance)\n",
    "                    p_key = \"%s --- %s\" % (g_key, p_key)\n",
    "                    \n",
    "                    if DEBUG is True: print \"p:\", key, p_instance            \n",
    "                    if DEBUG is True: print\n",
    "                    t = t + 1\n",
    "                    sp = \"-\".join(sorted([p_instance[0].lower(), p_instance[1].lower()]))\n",
    "                    sg = \"-\".join(sorted([g_instance[0].lower(), g_instance[1].lower()]))\n",
    "\n",
    "                    if DEBUG is True: print sg\n",
    "                    if DEBUG is True: print sp\n",
    "                    if sg == sp:\n",
    "                        ssn = ssn + 1 \n",
    "\n",
    "                    g_m_a = get_matches(g_instance[0], g_instance[2], expand=EXPAND)\n",
    "                    if DEBUG is True: print g_m_a\n",
    "                    g_m_b = get_matches(g_instance[1], g_instance[3], expand=EXPAND)\n",
    "                    if DEBUG is True: print g_m_b\n",
    "                    if DEBUG is True: print\n",
    "\n",
    "                    p_a = p_instance[2].lower()\n",
    "                    p_b = p_instance[3].lower()\n",
    "                    \n",
    "                    if p_a in g_m_a:\n",
    "                        if p_b in g_m_b:\n",
    "                            if DEBUG is True: print \"found match for a and b\"\n",
    "                            ab = ab + 1\n",
    "                            match_hash[p_key].append(\"ab\")\n",
    "                            record_instances(g_instance, p_instance, g_m_a, g_m_b, oab)\n",
    "                            if sg == sp: x = x + 1\n",
    "                            found_some_match_flag = True\n",
    "                        else:\n",
    "                            if DEBUG is True: print \"found match only for a\"\n",
    "                            a = a + 1\n",
    "                            match_hash[p_key].append(\"a\")\n",
    "                            record_instances(g_instance, p_instance, g_m_a, g_m_b, oa)\n",
    "                            found_some_match_flag = True\n",
    "                    elif p_a in g_m_b:\n",
    "                        if p_b in g_m_a:\n",
    "                            if DEBUG is True: print \"found match for a and b\"\n",
    "                            ab = ab + 1\n",
    "                            match_hash[p_key].append(\"ab\")\n",
    "                            record_instances(g_instance, p_instance, g_m_b, g_m_a, oab)\n",
    "                            if sg == sp: x = x + 1\n",
    "                            found_some_match_flag = True\n",
    "                        else:\n",
    "                            if DEBUG is True: print \"found match only for a\"                        \n",
    "                            a = a + 1\n",
    "                            match_hash[p_key].append(\"a\")\n",
    "                            record_instances(g_instance, p_instance, g_m_b, g_m_a, oa)\n",
    "                            found_some_match_flag = True                 \n",
    "                    else:\n",
    "                        if DEBUG is True: print \"found no match for either a or b\"\n",
    "                        record_instances(g_instance, p_instance, g_m_a, g_m_b, onab)\n",
    "                        nab = nab + 1           \n",
    "                        match_hash[p_key].append(\"nab\")\n",
    "\n",
    "    #             else:\n",
    "    #                 print \"found no match for either a or b\"\n",
    "    #                 nab = nab + 1\n",
    "\n",
    "                    if DEBUG is True: print\n",
    "                    if DEBUG is True: print \"-\"*100\n",
    "\n",
    "                    # we should break at the first match\n",
    "                    #if found_some_match_flag is True or found_some_relaxed_match_flag is True:\n",
    "                    #    break                \n",
    "\n",
    "#                 if found_some_match_flag is False:\n",
    "#                     if DEBUG is True: print \"found no match for either a or b\"\n",
    "#                     record_instances(g_instance, p_instance, g_m_a, g_m_b, onab)\n",
    "#                     nab = nab + 1           \n",
    "#                     match_hash[p_key].append(\"nab\")\n",
    "                    \n",
    "            else:\n",
    "                if DEBUG is True: print \"did not find key in predicted\"\n",
    "                np = np + 1                \n",
    "\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "        if i < 0:\n",
    "            break\n",
    "\n",
    "print \"Number of string matches and ID matches\", x\n",
    "print \"Number of string matches only          \", ssn\n",
    "print\n",
    "print \"Number of gold and predicted pairs compared\", t\n",
    "print\n",
    "print \"Number of unique document ID and Kd values in INTACT:\", gn\n",
    "print \"Number of                        Kd values in INTACT\", gnn\n",
    "print\n",
    "print \"Number of unique document ID and Kd values in PREDICTED INTACT:\", pn\n",
    "print \"Number of                        Kd values in PREDICTED INTACT\", pnn\n",
    "print\n",
    "if EXPAND == True:\n",
    "    print \"With Expansion:\"\n",
    "else:\n",
    "    print \"Without Expansion:\"\n",
    "print \"Number of times the key was     found in predicted\", pp\n",
    "print \"Number of times the key was not found in predicted\", np\n",
    "print\n",
    "print \"Number of times one of the two participant IDs matched\", a\n",
    "print \"Number of times           both participant IDs matched\", ab\n",
    "print \"Number of times    none of the participant IDs matched\", nab\n",
    "print\n",
    "print \"Number predicted instances compared to gold instances\", len(match_hash.keys())\n",
    "print \"(This should match the sum of earlier three possible cases)\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "context_hash = {}\n",
    "with codecs.open(\"../tasks/evaluate-extractions/affinity-values-with-context.txt\") as context:\n",
    "    for line in context:\n",
    "        key, value = line.split(\"__NEWLINE__\", 1)\n",
    "        context_hash[key] = value\n",
    "context_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "bundle = [[\"one\", \"a\"],\n",
    "            [\"both\", \"ab\"],\n",
    "            [\"neither\", \"nab\"]]\n",
    "\n",
    "for b in bundle:\n",
    "    fn, x = b[0], b[1]\n",
    "    with codecs.open(\"../tasks/evaluate-extractions/another-report---%s.txt\" % (fn), \"w\", \"utf-8\") as ar:\n",
    "        for key in sorted(match_hash.keys()):\n",
    "\n",
    "            if match_hash[key][0] == x:\n",
    "#                 print >>ar, key, match_hash[key]\n",
    "\n",
    "                g, p = key.split(\"---\")\n",
    "                g = g.strip()\n",
    "                print >>ar, \"g:\", g.strip().replace(\"--\", \", \")\n",
    "                g_bits = g.split(\"--\")\n",
    "                p = p.strip()\n",
    "                print >>ar, \"p:\", p.strip().replace(\"--\", \", \")\n",
    "                print >>ar, \"\"\n",
    "\n",
    "                print >>ar, \"\"\n",
    "                cards, s_indices = get_index_cards_for_pmid(g_bits[-1])\n",
    "#                 print >>ar, s_indices\n",
    "#                 print >>ar, cards\n",
    "                for key in cards:\n",
    "                    a, b, ga, gb = cards[key]\n",
    "                    ga = ga.replace(\"::\", \":\").lower()\n",
    "                    gb = gb.replace(\"::\", \":\").lower()\n",
    "\n",
    "                    if ga in human2uniprot_id_map:\n",
    "                        ga = human2uniprot_id_map[ga]\n",
    "\n",
    "                    if gb in human2uniprot_id_map:\n",
    "                        gb = human2uniprot_id_map[gb]\n",
    "\n",
    "                    print >>ar, key, \",\".join([a, b, ga, gb])\n",
    "                    print >>ar, \"\\n\"\n",
    "                    print >>ar, \"[gold]\", a, \"->\", get_matches(a, ga, expand=True)\n",
    "                    print >>ar, \"[gold]\", b, \"->\", get_matches(b, gb, expand=True)\n",
    "                    print >>ar, \"\\n\"\n",
    "\n",
    "                pk = p.strip().replace(\"--\", \",\").replace(\" \", \"@@\")\n",
    "                if pk in context_hash:\n",
    "                    print >>ar, context_hash[pk].replace(\"__NEWLINE__\", \"\\n\").replace(\"--------------------------------------------------------------------------------\", \"\").strip()\n",
    "                    print >>ar, \"-\"*100\n",
    "                else:\n",
    "                    print >>ar, \"[None]\"\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Sort the participants and resolve instances with conflicting Kd values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "import csvkit\n",
    "from collections import defaultdict\n",
    "\n",
    "sorted_uniq_fn = \"../tasks/evaluate-extractions/predicted-intact-affinities---sorted---uniq.v1.csv\"\n",
    "sorted_fn = \"../tasks/evaluate-extractions/predicted-intact-affinities.v1.csv\"\n",
    "\n",
    "reader = csvkit.reader(open(sorted_fn, \"r\"))\n",
    "header = reader.next()\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "#from l2k2r2.util import compare_tuple\n",
    "uniq_hash = defaultdict(list)\n",
    "\n",
    "for row in reader:    \n",
    "    a = (row[0], row[2])\n",
    "    b = (row[1], row[3])\n",
    "\n",
    "    pair = [a, b]\n",
    "    sorted_pair = sorted(pair, cmp=compare_tuple)\n",
    "\n",
    "    if DEBUG is True:\n",
    "        if pair != sorted_pair:\n",
    "            print pair\n",
    "            print sorted_pair\n",
    "            print\n",
    "\n",
    "    r = [sorted_pair[0][0], sorted_pair[1][0], sorted_pair[0][1], sorted_pair[1][1]] + row[4:]\n",
    "    uniq_hash[(sorted_pair[0][0], sorted_pair[1][0], sorted_pair[0][1], sorted_pair[1][1], row[-1])].append(row[4])\n",
    "\n",
    "with open(sorted_uniq_fn, \"w\") as sufn:\n",
    "    writer = csvkit.writer(sufn)\n",
    "    writer.writerow(header)        \n",
    "    \n",
    "    for key in uniq_hash:\n",
    "        if len(uniq_hash[key]) > 1:\n",
    "            for i, value in enumerate(uniq_hash[key]):\n",
    "                if value != \"-\":\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            value = uniq_hash[key][0]\n",
    "\n",
    "        print \"selected:\", key, value\n",
    "        writer.writerow([key[0], key[1], key[2], key[3], value, key[4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Sort the participants along with the IDs for another set of interactions without kd values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "import csvkit\n",
    "from collections import defaultdict\n",
    "\n",
    "sorted_uniq_fn = \"../all-with-context.sorted\"\n",
    "fn = \"../all-with-context\"\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "from l2k2r2.util import compare_tuple\n",
    "uniq_hash = defaultdict(list)\n",
    "context = []\n",
    "\n",
    "with codecs.open(fn, \"r\", \"utf-8\") as fn, codecs.open(sorted_uniq_fn, \"w\", \"utf-8\") as sufn:\n",
    "    for row in fn:\n",
    "        if row.startswith(\"pmid_\"):\n",
    "            inside_flag = True\n",
    "\n",
    "            row = row.replace(\"\\\"\", \"\")\n",
    "            row_bits = row.strip().split()\n",
    "    #         print row_bits\n",
    "            a = (row_bits[1], row_bits[3])\n",
    "            b = (row_bits[2], row_bits[4])\n",
    "\n",
    "            pair = [a, b]\n",
    "            sorted_pair = sorted(pair, cmp=compare_tuple)\n",
    "\n",
    "            if DEBUG is True:\n",
    "                if pair != sorted_pair:\n",
    "                    print pair\n",
    "                    print sorted_pair\n",
    "                    print\n",
    "\n",
    "            r = [row_bits[0]] + [sorted_pair[0][0], sorted_pair[1][0], sorted_pair[0][1], sorted_pair[1][1]]\n",
    "            print >>sufn, \" \".join(r)\n",
    "            \n",
    "            uniq_hash[(row_bits[0], sorted_pair[0][0], sorted_pair[1][0], sorted_pair[0][1], sorted_pair[1][1])].append(r)\n",
    "        else: \n",
    "            print >>sufn, row.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Creating a .html file with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#cat x | while read row; do echo $row | csvcut -c1-4,6 | sed 's/,pmid/,[^-][^-]\\*,pmid/g' | while read x; do echo $row; string=$(grep -P \"$x\" ../../python-notebooks/output.log | head -n 2 | awk -F\",\" '{print $5}' | sed 's/(/_LRB_/g; s/)/_RRB_/g'); grep -P \"$x\" ../../python-notebooks/output.log | head -n 1| sed 's/__NEWLINE__/\\n/g' | tail -n +2 | hilite \"$string\"; done; dline; done | tee affinity-values-with-context.txt && cat affinity-values-with-context.txt |  ../../scripts/ansi2html.sh --bg=dark > affinity-values-with-context.html && cp affinity-values-with-context.html /Users/pradhan/sshfs/cemantix/home/cemantix/public_html/documents/isi---big-mechanism/predicted-intact/\n",
    "\n",
    "# working\n",
    "cat x | while read row; do echo $row | csvcut -c1-4,6 | sed 's/,pmid/,[^-][^-]\\*,pmid/g' | while read x; do echo $row | regex -r \"\\n\" -R \"__NEWLINE__\" -; string=$(grep -P \"$x\" ../../python-notebooks/output.log | head -n 2 | awk -F\",\" '{print $5}' | sed \"s/(/_LRB_/g; s/)/_RRB_/g; s/'/./g\"); grep -P \"$x\" ../../python-notebooks/output.log | head -n 1 | sed 's/__NEWLINE__/\\n/g' | tail -n +2 | sed 's/(/_LAB_/g; s/)/_RAB_/g'  | if [[ -z \"$string\" ]]; then echo \"--------------------------------------------------------------------------------\"; else hilite \"$string\"; fi | regex -r \"\\n\" -R \"__NEWLINE__\" -; echo; done; done | tee affinity-values-with-context.txt\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#4c4c4c",
    "running_highlight": "#FF0000",
    "selected_highlight": "#4c4c4c"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "949px",
    "left": "0px",
    "right": "1850.45px",
    "top": "131px",
    "width": "418px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
