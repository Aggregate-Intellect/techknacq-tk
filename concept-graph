#!/usr/bin/env python3

# T: Concept Graph
# Jonathan Gordon

import sys
import os
import tempfile
import glob
import random
import subprocess
import re

from collections import defaultdict

from mallet import Mallet
from t.corpus import Corpus
from t.conceptgraph import ConceptGraph

# Parameters

MALLET_PATH = 'ext/mallet/bin/mallet'

LDA_TOPICS = 200
LDA_ITERATIONS = 1000

###

class TopicDependency:
    """Python wrapper class for Linhong's Java code for predicting dependencies
    among LDA topics."""

    jar = 'ext/techknacq-core/target/techknacq-core.jar'

    def __init__(self, corpus, model, method='ce'):
        self.corpus = corpus
        self.model = model

        rand_prefix = hex(random.randint(0, 0xffffff))[2:] + '-'
        self.prefix = os.path.join(tempfile.gettempdir(), rand_prefix)

        if os.path.exists('alledge.tsv'):
            print('Using existing alledge.tsv file.')
        else:
            doc_comp_file = self.convert_comp_file()
            citation_network_file = self.build_citation_network()
            infomap_tree_file, infomap_flow_file = self.get_infomap()

            deps = glob.glob('ext/techknacq-core/target/dependency/*.jar')

            cmd = ['java', '-cp', ':'.join([self.jar] + deps),
                   'edu.isi.techknacq.topics.graph.Comparisononalledges',
                   model.wtkfile, infomap_tree_file, doc_comp_file,
                   str(len(model.topics)), citation_network_file,
                   infomap_flow_file]

            if subprocess.call(cmd) != 0:
                sys.stderr.write('Running topic-dependency code failed.\n')
                sys.exit(1)

        self.read_alledges(method)

        #self.generate_topicgraph()

        try:
            #os.remove('alledge.tsv')
            os.remove('entropy1.txt')
            os.remove('entropy2.txt')
        except FileNotFoundError:
            pass


    def read_alledges(self, method='ce', threshhold=0.007):
        """Read the cross-entropy edges from the alledges.tsv output file."""

        self.edges = defaultdict(dict)
        for line in open('alledge.tsv'):
            try:
                tokens = line.strip().split()

                t1 = tokens[0]
                if t1 == 'sid': continue

                t2 = tokens[2]
                if method == 'simword':
                    weight = float(tokens[5])
                elif method == 'ce':
                    weight = float(tokens[7])
                elif method == 'avg':
                    weight = (float(tokens[5]) + float(tokens[7]))/2.0

                # Flip negative weights.
                if weight < 0:
                    weight = -1.0 * weight
                    t1, t2 = t2, t1
                if weight > threshhold:
                    self.edges[t1][t2] = max(self.edges[t1].get(t2, 0.0),
                                             weight)
            except:
                sys.stderr.write('Error processing alledges line:\n')
                sys.stderr.write(line)


    def generate_topicgraph(self):
        """Read the alledge.tsv file generated from techknacq-core and
        produce the topicgraph.txt file used by techknacq-server."""

        with open('topicgraph.txt', 'w') as fout:
            fout.write('%d\n' % (len(self.edges)))
            for t1 in self.edges:
                good_edges = sorted([(y, self.edges[t1][y])
                                     for y in self.edges[t1]],
                                    key=lambda x: self.edges[t1][x],
                                    reverse=True)
                fout.write('%s,%d' % (t1, len(good_edges)))
                for t2, w in good_edges:
                    fout.write(':%s,%s' % (t2, w))
                fout.write('\n')


    def convert_comp_file(self):
        """Convert Mallet composition file to the format needed by the
        topic dependency code."""

        fname = self.prefix + 'concept2doc.txt'

        def pairwise(iterable):
            """s -> (s0,s1), (s2,s3), (s4, s5), ..."""
            a = iter(iterable)
            return zip(a, a)

        with open(fname, 'w') as fout:
            for line in open(self.model.dtfile):
                if line[0] == '#':
                    continue
                elts = line.strip().split('\t')[1:]
                fout.write(elts[0].replace('file:', ''))
                for tid, pct in sorted(pairwise(elts[1:]),
                                       key=lambda x: x[0]):
                    fout.write('\ttopic' + tid + ':' + pct)
                fout.write('\n')

        return fname


    def get_infomap(self):
        treefname = self.prefix + 'infomap-tree.txt'
        flowfname = self.prefix + 'infomap-flow.txt'

        # Generate Linhong-style names for each topic.
        keynames = []
        for t in self.model.topics:
            keynames.append('-'.join([x[0] for x in
                                      sorted(t, key=lambda x: x[1],
                                             reverse=True)[:5]]) + '-')

        # Generate input file for Infomap.
        with open(self.prefix + 'infomap.net', 'w') as fout:
            fout.write('*Vertices %d\n' % (len(self.model.topics)))
            for v in range(len(self.model.topics)):
                fout.write('%d "%s"\n' % (v, keynames[v]))
            out_text = ''
            edge_count = 0
            for i, line in enumerate(open(self.model.cofile)):
                for j, count in enumerate(re.split('[^0-9]+', line.strip())):
                    if int(count) > 0 and j > i:
                        out_text += '%d %d %s\n' % (i, j, count)
                        edge_count += 1
            fout.write('*Edges %d\n' % (edge_count))
            fout.write(out_text)

        cmd = ['ext/infomap/Infomap', '-z',
               '--flow-network', self.prefix + 'infomap.net',
               tempfile.gettempdir()]
        if subprocess.call(cmd) != 0:
            sys.stderr.write('Running Infomap failed.\n')
            sys.exit(1)

        os.remove(self.prefix + 'infomap.net')

        os.rename(self.prefix + 'infomap.tree', treefname)
        os.rename(self.prefix + 'infomap.flow', flowfname)

        return treefname, flowfname


    def build_citation_network(self):
        fname = self.prefix + 'cite.txt'

        with open(fname, 'w') as fout:
            for doc in self.corpus.docs:
                for ref in doc.references:
                    fout.write('%s ==> %s\n' % (doc.id, ref))

        return fname

###


if __name__ == '__main__':
    if len(sys.argv) != 2 and len(sys.argv) != 3:
        print('Usage: concept-graph [corpus dir.]', file=sys.stderr)
        print('    or concept-graph [corpus dir.] [topic model path+prefix]',
              file=sys.stderr)
        sys.exit(1)

    rand_prefix = hex(random.randint(0, 0xffffff))[2:] + '-'
    prefix = os.path.join(tempfile.gettempdir(), rand_prefix)

    cg = ConceptGraph()

    corpus = Corpus(sys.argv[1])
    #corpus.fix_text()

    cg.add_docs(corpus)

    if len(sys.argv) == 2:
        print('Generating topic model.')
        mallet_corpus = prefix + 'corpus'
        os.makedirs(mallet_corpus)
        corpus.export(mallet_corpus, abstract=False, format='text')
        model = Mallet(MALLET_PATH, mallet_corpus, num_topics=LDA_TOPICS,
                       iters=LDA_ITERATIONS, bigrams=True)
    else:
        print('Loading topic model.')
        model = Mallet(MALLET_PATH, prefix=sys.argv[2])
        if len(model.topic_doc[0]) < len(corpus.docs):
            print('Found more documents in the corpus (%d)' %
                  (len(corpus.docs)), end=' ')
            print('than in the topic model (%d).' % (len(model.topic_doc[0])))
            print('Inferring topics for corpus documents.')
            mallet_corpus = prefix + 'corpus'
            os.makedirs(mallet_corpus)
            corpus.export(mallet_corpus, abstract=False, format='text')
            model.infer_topics(mallet_corpus)

    cg.add_concepts(model)

    dep = TopicDependency(corpus, model, method='avg')
    cg.add_dependencies(dep.edges)

    cg.export(prefix + 'cg.json')
